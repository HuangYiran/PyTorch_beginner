{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "这里不但包括了单词的embeddings，同时还包括了各种其他的属性的embedding和position的。<br>\n",
    "不同feature之间存在三种融合的方式，即concat，sum，mlp。<br>\n",
    "模型是由多个nn.Embedding模型(针对不同的features)组合而成，根据输入和参数的不同，呈现不同数目及大小的模型。<br>\n",
    "输出的embedding已经融合了所要求的features\n",
    "#### 属性\n",
    "- self.positional_encoding: Boolean, action = 'store_true'是否使用positional_encoding.\n",
    "- self.pe: [max_len, 1, word_vec_len] max_len是最大可能的位置数量。保存的代表不同位置的向量。使向量长度为词向量的长度，是为了方便和单词的词向量进行结合。\n",
    "- self.feat_merge: choices=['concat', 'sum', 'mlp']。指定不同features embedding之间的融合方式\n",
    "- feat_exp = opt.feat_vec_exponent: 指定如果merge方式使用concat的话，融合后的向量长度，应该为对应feature的值域空间的feat_exp次方。default = 0.7\n",
    "- vocab_sizes：列表，指各种属性的值域空间的大小。\n",
    "- emb_sizes: 列表，指各种属性生成的向量的长度大小。\n",
    "- self.mlp: 当使用mlp融合时，mlp的结构\n",
    "- self.emb_luts: nn.ModuleList保存各个feature的embedding模型。\n",
    "\n",
    "#### 方法\n",
    "- __init__(self, opt, dicts, feature_dicts = None): 设置dict，feature dict的embedding模型。\n",
    "- word_lut(self): 输出单词的embedding模型\n",
    "- embedding_size(self): 返回词向量的长度\n",
    "- make_positional_encoding(self, dim, mac_len): 生成positional embedding\n",
    "- load_pretrained_vectors(self, emb_file): 顾名思义\n",
    "- merge(self, features): 融合\n",
    "- forward(self, src_input_): 运行方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "设定MT系统的Encoder部分，可以调节层的数量及每个层中节点的数量以及节点的类型。模型输入由前置的embedding模型得到。<br>\n",
    "目前支持三种类型的层：即GRU，LSTM和Transformer。<br>\n",
    "#### 属性\n",
    "- self.layers: int, encoder层的个数\n",
    "- self.num_directions: int [0, 1] 是否bidirection\n",
    "- self.hidden_size: 每个隐藏层的节点的数量，这里指的是每个方向的数量，如果是bidirection，那么总的数量应该是它的两倍。\n",
    "- self.embeddings：embedding模型\n",
    "- self.encoder_layer: choices = ['GRU', 'LSTM', 'Transformer'] 指定每个层的用的结构类型\n",
    "- self.transformer: 如果encoder_layer是Transformer，设定为对应的transformer模型（onmt.modules.TransformerEncoder）列表（每一项代表一层）\n",
    "- self.rnn: 如果encoder_layer不是Transformer，设定为对应的rnn模型（nn.LSTM/nn.GRU）列表\n",
    "\n",
    "#### assert\n",
    "assert opt.rnn_size % self.num_direction == 0\n",
    "\n",
    "#### 方法\n",
    "__init__(self, opt, dicts, feature_dicts = None)\n",
    "forward(self, intput, lengths = None, hidden = None): length是指batch_size，hidden是隐藏层的初始值。\n",
    "\n",
    "#### 图\n",
    "还木画"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "设定MT模型的Decoder部分，可以设定层的数量及每个层中节点的数量以及节点的类型，attention的类型。<br>\n",
    "目前支持的层的类型有：GRU，LSTM，Transformer<br>\n",
    "目前支持的attention的类型有：dot, general, mlp, copy<br>\n",
    "目前支持的结合context的方式有：source, target, both<br>\n",
    "\n",
    "#### 属性\n",
    "- self.layers: int, decoder层的个数，encoder和decoder层数相同\n",
    "- self.decoder_layer: int, decoder每个层的类型\n",
    "- self._coverage: action='store_true' 指定attention是否要使用coverage。用额外的向量来记录已经翻译到什么地方，从而实现coverage。\n",
    "- self.hidden_size: 因为不用考虑bidrectional所以等于opt.rnn_size\n",
    "- self.input_feed: 指定是否使用额外的context信息作为输入信息，如果是将扩充input_size(原始为word_vec_size)，default = 1\n",
    "- self.transformer: 如果decoder_layer是Transformer，设定为对应的Transformer模型（onmt.modules.TransformerDecoder()）列表\n",
    "- self.rnn: 如果decoder_layer不是Transformer，设定为对应的rnn模型(onmt.modules.StackedLSTM/GRU)列表\n",
    "- self.context_gate: 指定使用的context_gate的类型，choices = ['source', 'target', 'both']。当使用input_feed时，context_gate作为权值，权衡contxt和原始input之间的比重\n",
    "- self.dropout: dropput模型。\n",
    "- self.attn: attention模型，可以指定attention的类型，choices = ['dot', 'general', 'mlp']\n",
    "- self._copy: 指定是否训练copy attention模型（onmt.modules.GlobalAttention）， action = 'store_true'\n",
    "\n",
    "#### 方法\n",
    "- __init__(self, opt, dicts)<br>\n",
    "- forward(self, input, src, context, state): context [src_len, batch, rnn_size], state是一个用于初始化decode的对象\n",
    "\n",
    "#### 图\n",
    "还木画"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMTModel\n",
    "\n",
    "#### 属性\n",
    "- self.multigpu: boolean是否使用multi gpu\n",
    "- self.encoder\n",
    "- self.decoder\n",
    "\n",
    "#### 方法\n",
    "- __init__(self, encoder, decoder, multigpu = False)\n",
    "- _fix_enc_hidden(sellf, h): change [layers*directions, batch, dim] to [layers, batch, directions*dim]\n",
    "- init_decoder_state(self, context, enc_hidden)\n",
    "- forward(self, src, tgt, lengths, dec_state = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNDecoderState(DecoderState)\n",
    "\n",
    "#### 属性\n",
    "self.hidden: rnn中的可训练变量[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dict features不是很了解，有什么features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题\n",
    "copy attention并不知道是什么东西"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
