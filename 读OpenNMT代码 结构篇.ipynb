{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "这里不但包括了单词的embeddings，同时还包括了各种其他的属性的embedding和position的。<br>\n",
    "不同feature之间存在三种融合的方式，即concat，sum，mlp。<br>\n",
    "模型是由多个nn.Embedding模型(针对不同的features)组合而成，根据输入和参数的不同，呈现不同数目及大小的模型。<br>\n",
    "输出的embedding已经融合了所要求的features\n",
    "#### 属性\n",
    "- self.positional_encoding: Boolean, action = 'store_true'是否使用positional_encoding.\n",
    "- self.pe: [max_len, 1, word_vec_len] max_len是最大可能的位置数量。保存的代表不同位置的向量。使向量长度为词向量的长度，是为了方便和单词的词向量进行结合。\n",
    "- self.feat_merge: choices=['concat', 'sum', 'mlp']。指定不同features embedding之间的融合方式\n",
    "- feat_exp = opt.feat_vec_exponent: 指定如果merge方式使用concat的话，融合后的向量长度，应该为对应feature的值域空间的feat_exp次方。default = 0.7\n",
    "- vocab_sizes：列表，指各种属性的值域空间的大小。\n",
    "- emb_sizes: 列表，指各种属性生成的向量的长度大小。\n",
    "- self.mlp: 当使用mlp融合时，mlp的结构\n",
    "- self.emb_luts: nn.ModuleList保存各个feature的embedding模型。\n",
    "\n",
    "#### 方法\n",
    "- __init__(self, opt, dicts, feature_dicts = None): 设置dict，feature dict的embedding模型。\n",
    "- word_lut(self): 输出单词的embedding模型\n",
    "- embedding_size(self): 返回词向量的长度\n",
    "- make_positional_encoding(self, dim, mac_len): 生成positional embedding\n",
    "- load_pretrained_vectors(self, emb_file): 顾名思义\n",
    "- merge(self, features): 融合\n",
    "- forward(self, src_input_): 运行方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "设定MT系统的Encoder部分，可以调节层的数量及每个层中节点的数量以及节点的类型。模型输入由前置的embedding模型得到。<br>\n",
    "目前支持三种类型的层：即GRU，LSTM和Transformer。<br>\n",
    "#### 属性\n",
    "- self.layers: int, encoder层的个数\n",
    "- self.num_directions: int [0, 1] 是否bidirection\n",
    "- self.hidden_size: 每个隐藏层的节点的数量，这里指的是每个方向的数量，如果是bidirection，那么总的数量应该是它的两倍。\n",
    "- self.embeddings：embedding模型\n",
    "- self.encoder_layer: choices = ['GRU', 'LSTM', 'Transformer'] 指定每个层的用的结构类型\n",
    "- self.transformer: 如果encoder_layer是Transformer，设定为对应的transformer模型（onmt.modules.TransformerEncoder）列表（每一项代表一层）\n",
    "- self.rnn: 如果encoder_layer不是Transformer，设定为对应的rnn模型（nn.LSTM/nn.GRU）列表\n",
    "\n",
    "#### assert\n",
    "assert opt.rnn_size % self.num_direction == 0\n",
    "\n",
    "#### 方法\n",
    "__init__(self, opt, dicts, feature_dicts = None)\n",
    "forward(self, intput, lengths = None, hidden = None): length是指batch_size，hidden是隐藏层的初始值。\n",
    "\n",
    "#### 图\n",
    "还木画"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "设定MT模型的Decoder部分，可以设定层的数量及每个层中节点的数量以及节点的类型，attention的类型。<br>\n",
    "目前支持的层的类型有：GRU，LSTM，Transformer<br>\n",
    "目前支持的attention的类型有：dot, general, mlp, copy<br>\n",
    "目前支持的结合context的方式有：source, target, both<br>\n",
    "\n",
    "#### 属性\n",
    "- self.layers: int, decoder层的个数，encoder和decoder层数相同\n",
    "- self.decoder_layer: int, decoder每个层的类型\n",
    "- self._coverage: action='store_true' 指定attention是否要使用coverage。用额外的向量来记录已经翻译到什么地方，从而实现coverage。\n",
    "- self.hidden_size: 因为不用考虑bidrectional所以等于opt.rnn_size\n",
    "- self.input_feed: 指定是否使用额外的context信息作为输入信息，如果是将扩充input_size(原始为word_vec_size)，default = 1\n",
    "- self.transformer: 如果decoder_layer是Transformer，设定为对应的Transformer模型（onmt.modules.TransformerDecoder()）列表\n",
    "- self.rnn: 如果decoder_layer不是Transformer，设定为对应的rnn模型(onmt.modules.StackedLSTM/GRU)列表\n",
    "- self.context_gate: 指定使用的context_gate的类型，choices = ['source', 'target', 'both']。当使用input_feed时，context_gate作为权值，权衡contxt和原始input之间的比重\n",
    "- self.dropout: dropput模型。\n",
    "- self.attn: attention模型，可以指定attention的类型，choices = ['dot', 'general', 'mlp']\n",
    "- self._copy: 指定是否训练copy attention模型（onmt.modules.GlobalAttention）， action = 'store_true'\n",
    "\n",
    "#### 方法\n",
    "- __init__(self, opt, dicts)<br>\n",
    "- forward(self, input, src, context, state): context [src_len, batch, rnn_size], state是一个用于初始化decode的对象\n",
    "\n",
    "#### 图\n",
    "还木画"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMTModel\n",
    "\n",
    "#### 属性\n",
    "- self.multigpu: boolean是否使用multi gpu\n",
    "- self.encoder\n",
    "- self.decoder\n",
    "\n",
    "#### 方法\n",
    "- __init__(self, encoder, decoder, multigpu = False)\n",
    "- _fix_enc_hidden(sellf, h): change [layers*directions, batch, dim] to [layers, batch, directions*dim]\n",
    "- init_decoder_state(self, context, enc_hidden)\n",
    "- forward(self, src, tgt, lengths, dec_state = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNDecoderState(DecoderState)\n",
    "\n",
    "#### 属性\n",
    "self.hidden: rnn中的可训练变量[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dict features不是很了解，有什么features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## onmt.Loss.py\n",
    "### NMTCriterion\n",
    "设置loss function为NLLLoss，初始化weight使得忽略对应特殊字符。\n",
    "#### 参数\n",
    "- vocabulary Size\n",
    "- opt\n",
    "#### 返回\n",
    "Loss function\n",
    "\n",
    "### ShardVariable\n",
    "？？？？不知道是要干什么的？？？\n",
    "\n",
    "### collectGrads\n",
    "？？？？搞不懂啊，和上一个方法相关，等遇到了在看？？？\n",
    "\n",
    "### class Statistics\n",
    "计算各种统计数据\n",
    "#### 初始化参数\n",
    "- loss = 0 \n",
    "- n_words = 0\n",
    "- n_correct = 0\n",
    "\n",
    "#### 属性(都表示字面意思)\n",
    "- self.loss = loss\n",
    "- self.n_words = n_words\n",
    "- self.n_correct = n_correct\n",
    "- self.n_str_words = 0\n",
    "- self.start_time = time.time() (import time)\n",
    "\n",
    "#### 方法\n",
    "- _init_(self, loss = 0, n_words = 0, n_correct = 0):\n",
    "- update(self, stat): stat应该是statistics类型的，把stat中的属性，加到目前的属性中\n",
    "- accuracy(self): 返回$100*(self.n_correct / float(self.n_words)$\n",
    "- ppl(self): 返回$math.exp(min(self.loss/ self.n_words, 100))$\n",
    "- elapsed_time(self): 放回当前时间减去刚生成这个实例时的时间\n",
    "- output(self, epoch, batch, n_batches, start): 统计前面方法中得到的信息，进行输出\n",
    "- log(self, prefix, experiment, optim): 把信息加到experiment中\n",
    "\n",
    "### MemoryEfficientLoss\n",
    "这个先跳了？？？？？应该先看一下什么是copygenerator，否则实在看不懂啊？？？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## onmt.modules.CopyGenerator\n",
    "###  class CopyGenerator(nn.Module)\n",
    "暂时不知道为什么需要copyGenerator<br>\n",
    "Generator module that additionally considers copying words directly from the source\n",
    "#### 属性\n",
    "- self.linear: [rnn_size, tgt_dict.size()]，用于计算目标词汇表各个单词的概率\n",
    "- self.linear_copy: [rnn_size, 1]，用于计算copy的概率\n",
    "- self.src_dict\n",
    "- self.tgt_dict: target dict\n",
    "\n",
    "#### 方法\n",
    "- __init__(self, opt, src_dict, tgt_dict)\n",
    "- forward(self, hidden, attn, verbose = False)： args:hidden(FLoatTensor): (tgt_len \\* batch) x hidden为什么是tgt_length \\* batch，tgt_length是什么？？怀疑他表示目标句子的长度<br>\n",
    "计算$P(w) = p(z=1)p_{copy}(w|z=0) 和 p(z=0)*P_{softmax}(w|z=0)$，其中p(z=1)表示copy的概率，其值由输入hidden确定。$p_{softmax}$表示目标词汇表中，各个单词出现的概率，其值由输入hidden确定。$p_{copy}$就是输入attn：(tgt_len*length) x src_len\n",
    "- _debug_copy(self, src, copy, prob, out_prob, attn, mal_attn): 不是很懂这个方法是为了干什么？？？\n",
    "\n",
    "### CopyCriterion(probs, attn, targ, align, eps = 1e-12): 看不懂啊？？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## onmt.optim.py\n",
    "### class Optim(object)\n",
    "- self.last_ppl: 当前的ppl值\n",
    "- self.lr: learning rate\n",
    "- self.max_grad_norm: 设置max_grad_norm，用于clip_grad_norm.\n",
    "- self.method: 方法\n",
    "- self.lr_decay: learning rate decay??\n",
    "- self.start_decay_at\n",
    "- self.start_decay: boolean\n",
    "- self._step: 记录目前step的次数，用于辅助计算decay_lr，具体原理不懂？\n",
    "- self.betas: = [beta1, beta2]用于Adam。coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999))\n",
    "- self.opt: opt\n",
    "- self.optimizer: 优化器\n",
    "#### 方法\n",
    "- __init__(self, method, lr, max_grad_norm, lr_decay = 1, start_decay_at = None, beta1 = 0.9, beta2 = 9.8, opt = None)\n",
    "- set_parameters(self, params): 设置optimizer的方法，包括：sgd, adagrad, adadelta, adam\n",
    "- _setRate(self, lr): 设置optimizer的learning rate\n",
    "- step(self): 加入了根据是否设置了decay_method, 调整learning rate\n",
    "- updateLearningRate(self, ppl, epoch): decay learning rate if val perf does not improve or we hit the start_decay_at limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## onmt.IO.py\n",
    "### extractFeatures(tokens)\n",
    "输入的tokens是一个列表，其中每个项表示单词及其对应的词性，他们通过‘|’分隔开。该方法的功能主要是提取每个token中的word和features。<br>\n",
    "应该注意的是，每个单词可以有多个features，但是每个单词的features的总数应该是相同的。<br>\n",
    "该方法返回words, features(列表，len(features)代表feature的个数), numFeatures\n",
    "\n",
    "### merge_vocabs(vocabs, vocab_size = None)\n",
    "合并多个词汇表，一般词汇表从特定的文档汇中，归纳得到，不同文档得到的词汇表之间，存在差异。这个方法，主要用来合并这些词汇表。vocab_size指定最后生成的词汇表的大小，None的话就是没有限定。<br>\n",
    "用try: merged[word] += count except: merged[word] = 0，也是可以的，但不知道优劣在哪里。<br>\n",
    "返回用到torchtext，不知道是在干什么？？还有vocab也是torchtext里面的，所以只能表示懵逼了？？？\n",
    "\n",
    "### class OrderedIterator(torchtext.data.Iterator)\n",
    "父类是什么鬼？？？\n",
    "\n",
    "### class ONMTDataset(torchtext.data.Dataset):\n",
    "父类是什么鬼？？？\n",
    "以torchtext.data.Dataset为基础，打造一个使用与MT任务的Dataset类。\n",
    "#### 外部方法有\n",
    "collapseCopyScores(self, scores, batch, tgt_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train.py\n",
    "#### 读取参数并做一些设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//提取选项\n",
    "parser = argparse.ArgumentParser(description='train.py')\n",
    "opt = parser.parse_args() \n",
    "\n",
    "// 设定层数和embedding vector的长度。\n",
    "// 若没有特殊指定，默认为2和500\n",
    "opt.src_word_vec_size = opt.tgt_word_vec_size = opt.word_vec_size\n",
    "opt.enc_layers = opt.dec_layers = opt.layers\n",
    "\n",
    "// 设定是否用双向encoder\n",
    "opt.brnn = (opt.encoder_type == 'brnn')\n",
    "\n",
    "// 设定seed，并不知道这一步，优在哪里？？？\n",
    "if opt.seed>0: torch.manual_seed(opt.seed)\n",
    "    \n",
    "// 若存在n卡，那么最好就用显卡跑吧，关于显卡的一切，目前表示未知？？？\n",
    "if torch.cuda.is_available() and opt.gpuid: \n",
    "    cuda.set_device(opt.gpuid[0]) \n",
    "    if opt.seed > 0: torch.cuda.manual_seed(opt.seed)\n",
    "        \n",
    "// 设定远端log服务器crayon，若当前experiment已经存在，则覆盖之。具体使用同样表示未知？？？\n",
    "if opt.exp_host != \"\":\n",
    "    from pycrayon import CrayonClient\n",
    "    cc = CrayonClient(hostname = opt.exp_host)\n",
    "    if opt.exp in cc.get_experiment_names(): cc.remove_experiment(opt.exp)\n",
    "    experiment = cc. create_experiment(opt.exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实现main方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-7d9699878920>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-7d9699878920>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    checkpoint = troch.load(dict_checkpoint, map_location = lambda storage, loc: storage) //为什么这么remap\u001b[0m\n\u001b[0m                                                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 读取数据到cpu, 设置一些变量。不知道什么样的数据才能使用这种方法读取，只有通过torch.store的数据吗？\n",
    "# 不是很清楚这些数据是用来干嘛的，估计train存储了训练用的数据，field存储了训练相关的一些信息，像词汇表之类的。\n",
    "# 但是为什么要一次性的读取到cpu呢，对于这点表示不解，特别是训练数据，不怕太大吗？？？\n",
    "train = torch.load(opt.data + '.train.pt', pickle_module = dill)\n",
    "field = torch.load(opt.data + '.fields.pt', pickle_module = dill)\n",
    "valid = torch.load(opt.data + '.valid.pt', pickle_module = dill)\n",
    "fields = dict(fields)\n",
    "src_features = [fields[\"src_feat_\"+ str(j)] for j in range(train.nfeatures)]\n",
    "checkpoint = None\n",
    "\n",
    "# 如果checkpoint中有训练信息，则从中读取\n",
    "dict_checkpoint = opt.train_from\n",
    "if dict_checkpoint:\n",
    "    checkpoint = troch.load(dict_checkpoint, map_location = lambda storage, loc: storage) //为什么这么remap\n",
    "    fields = checkpoint['field']\n",
    "    \n",
    "# 从下面两个输出可以看出不少东西：\n",
    "# fields主要包括src,tgt和各种feature记录, vocab表示对应项的值域，比如src就是源词汇表。\n",
    "# 一个问题是这些src_feature是从checkpoint中，还是opt.data中得来的呢？？？？？？\n",
    "print(' * vocabulary size. source = %d; target = %d'%(len(fields['src'].vocab), len(fields['tgt'].vocab)))\n",
    "for j, feat in enumerate(src_features):\n",
    "    print(' * src feature %d size = %d' %(j, len(feat.vocab)))\n",
    "# 证实train确实是训练数据\n",
    "print('* number of training sentences. %d'%len(train))\n",
    "print('* maxium batch size. %d' % opt.batch_size)\n",
    "\n",
    "\"\"\" 下面开始建模，主要操作包括\n",
    "1. 分析是否有src_feature，有的话embedding的时候应该加入考虑\n",
    "2. 根据opt决定encoder和decoder，并构建NMT模型\n",
    "3. 决定是否训练copy attention层，这个不是很了解，没见过？？？？？？\n",
    "4. 是否有checkpoint，有的话就读取参数\n",
    "5. 是否显卡运行\n",
    "\"\"\"\n",
    "cuda = (len(opt.gpuid) >= 1)\n",
    "model = onmt.Models.make_base_model(opt, opt, fields, cuda, checkpoint)\n",
    "\n",
    "# 如果读取了checkpoint，调整起始epoch, 然而模型的读取并不是在这里进行\n",
    "if opt.train_from:\n",
    "    opt.start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "# 如果存在多快显卡，设置并行\n",
    "if len(opt.gpuid) > 1:\n",
    "    model = nn.DataParallel(model, device_ids = opt.gupid, dim = 1)\n",
    "\n",
    "\"\"\" 如果没有从checkpoint读取数据，那么就进行初始化\n",
    "1. 使用uniform distribution进行初始化，如果param_init为了0，则取消初始化，否则使用uniform(-param_init, param_init)\n",
    "2. 如果提前训练有embedding，则直接读取\n",
    "3. 设置优化器(如果存在checkpoint，则直接读取)估计是合作编程，这里写得有点乱\n",
    "\"\"\"\n",
    "if not opt.train_from:\n",
    "    if opt.param_init != 0.0:\n",
    "        for p in model.parameter():\n",
    "            p.data.uniform_(-opt.param_init, opt.param_init)\n",
    "    model.encoder.embeddings.load_pretrained_vectors(opt.pre_word_vecs_enc)\n",
    "    model.decoder.embeddings.load_pretrained_vectors(opt.pre_word_vecs_dec)\n",
    "    optim = onmt.Optim(\n",
    "        opt.optim, opt.learning_rate, opt_max_grad_norm, \n",
    "        lr_decay = opt.learning_rate_decay,\n",
    "        start_decay_at = opt.start_decay_at,\n",
    "        opt = opt\n",
    "    )\n",
    "else:\n",
    "    optim = checkpoint['optim']\n",
    "optim.set_parameters(model.parameters())\n",
    "\n",
    "# 训练模型\n",
    "trainModel(model, train, valid, fields, optim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2-tf",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
