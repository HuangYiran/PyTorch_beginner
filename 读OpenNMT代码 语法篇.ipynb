{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本上没有什么顺序可言。模型都是比较熟悉的。但因为使用PyTorch，所以很多语法都不熟悉，所以就是把觉得比较新奇的语句给摘出来。<br>\n",
    "<br>\n",
    "一直搞不懂的是，为什么这些类明明没有实现__call__方法，但是都可以直接调用，而且好像默认调用了其中的forward方法。是因为nn.Modul中实现了__call__方法，而其中就有调用forward这个语句吗？？？有时间应该去看看源码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### argparse.ArgumentParser\n",
    "是python的一个命令行解析包。相当于tensorflow中的FLAGS吧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-v VERBOSITY] [-v2] [-x X] [-v3 {0,1,2}]\n",
      "                             echo\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/py3-tf/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"echo\") # 定义一个叫echo的参数，默认必选\n",
    "# 定义了可选参数-v或--verbosity，通过解析后，其值保存在args.verbosity变量中\n",
    "parser.add_argument(\"-v\", \"--verbosity\", help = \"increase output verbosity\")\n",
    "# 默认为True，不出现则为False\n",
    "parser.add_argument(\"-v2\", \"--verbosity2\", help = \"increase output verbosity\", action = \"store_true\")\n",
    "# argparse提供了对参数类型的解析，如果类型不符合，则直接报错\n",
    "parser.add_argument(\"-x\", type = int, help = \"the base\")\n",
    "# 可以设置可选值\n",
    "parser.add_argument(\"-v3\", \"--verbosity3\", type = int, choices = [0, 1, 2], help = \"increase output verbosity\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "if args.verbosity3 == 2:\n",
    "    print(\"th\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "主要确定：\n",
    "- 是否bidirection\n",
    "- 层的数量\n",
    "- 每层节点的数量\n",
    "- 使用什么层结果RNN，LSTM还是Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### assert opt.rnn_size % self.num_directions == 0\n",
    "使用assert断言是学习python一个非常好的习惯，python assert 断言句语格式及用法很简单。在没完善一个程序之前，我们不知道程序在哪里会出错，与其让它在运行最崩溃，不如在出现错误条件时就崩溃，这时候就需要assert断言的帮助。<br>\n",
    "python assert断言是声明其布尔值必须为真的判定，如果发生异常就说明表达示为假。可以理解assert断言语句为raise-if-not，用来测试表示式，其返回值为假，就会触发异常。<br>\n",
    "assert的异常参数，其实就是在断言表达式后添加字符串信息，用来解释断言并更好的知道是哪里出了问题。格式如下：<br>\n",
    "assert expression [, arguments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "坑爹啊",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6022c7154b0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"坑爹啊\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: 坑爹啊"
     ]
    }
   ],
   "source": [
    "assert 1==2, \"坑爹啊\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### self.hidden_size = opt.rnn_size // self.num_directions\n",
    "opt.rnn_size: size of LSTM hidden states<br>\n",
    "python 2.x里面，// 是地板除，/如果有一个数是浮点数就得到小数，如果两个都是整数也是地板除。<br>\n",
    "python 3.x里面，// 是地板除，/ 不管两边是不是整数得到的都是小数。<br>\n",
    "四舍五入请用: round(5/3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1.6666666666666667, 2, 1.67)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 5 // 3 \n",
    "b = 5 / 3\n",
    "c = round(5/3)\n",
    "d = round(5/3, 2)\n",
    "a, b, c, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.ModuleList(Modules = None)\n",
    "nn.ModuleList(<br>\n",
    "&emsp;[onmt.modules.TransformerEncoder(self.hidden_size, opt)<br>\n",
    "&emsp;for i in range(opt.layers)])<br>\n",
    "transformerEncoder实现每层由两部分组成：self-attention层和positionwiseFeedForward（bottleLinear? + ReLu + dropout + residual）层<br>\n",
    "Holds submodules in a list.<br>\n",
    "ModuleList can be indexed like a regular Python list, but modules it contains are properly registered, and will be visible by all Module methods.<br>\n",
    "Parameters:\tmodules (list, optional) – a list of modules to add<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### self.rnn = getattr(nn, opt.run_type)(...)\n",
    "getattr(object, name[, default])<br>\n",
    "Return the value of the named attribute of object. name must be a string. If the string is the name of one of the object’s attributes, the result is the value of that attribute. For example, getattr(x, 'foobar') is equivalent to x.foobar. If the named attribute does not exist, default is returned if provided, otherwise AttributeError is raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-ed73608e3478>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-ed73608e3478>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    if self.encoder_layer = \"transformer\":\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 这样搭网络感觉牛牛哒。 另外nn.Module还是很陌生，\n",
    "if self.encoder_layer = \"transformer\":\n",
    "    self.transformer = nn.ModuleList(\n",
    "        [onmt.modules.TransformerEncoder(self.hidden_size, opt)\n",
    "         for i in range(opt.layers)])\n",
    "else:\n",
    "    self.rnn = getattr(nn, opt.rnn_type)(\n",
    "        input_size, self.hidden_size, num_layers = out.layers,\n",
    "        dropout = out.dropout,\n",
    "        bidirectional = opt.brnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pad_attn_mask = seq_k.data.eq(onmt.Constants.PAD).unsequeeze(1).expand(mb_size, len_q, len_k)\n",
    "- torch.eq(input, other, out=None) → Tensor: <br>\n",
    "\n",
    "Computes element-wise equality. The second argument can be a number or a tensor whose shape is broadcastable with the first argument.<br>\n",
    "return: a torch.ByteTensor containing a 1 at each location where the tensors are equal and a 0 at every other location<br>\n",
    "- torch.unsqueeze(input, dim, out=None)<br>\n",
    "\n",
    "Returns a new tensor with a dimension of size one inserted at the specified position. The returned tensor shares the same underlying data with this tensor.<br>\n",
    "A negative dim value can be used and will correspond to dim+input.dim()+1<br>\n",
    "- expand(tensor, sizes) → Tensor<br>\n",
    "\n",
    "Returns a new view of the tensor with singleton dimensions expanded to a larger size.<br>\n",
    "Tensor can be also expanded to a larger number of dimensions, and the new ones will be appended at the front.<br>\n",
    "Expanding a tensor does not allocate new memory, but only creates a new view on the existing tensor where a dimension of size one is expanded to a larger size by setting the stride to 0. Any dimension of size 1 can be expanded to an arbitrary value without allocating new memory.<br>\n",
    "这是一个生成mask的代码。torch.eq找出向量中的特殊单词（Constants.PAD），unsequeeze是增加一个维，该维的长度为1，expand扩展（复制）该mask的维度扩展成给定的维度。因为这个mask是给matmal(Q,K)用的，所以其维度[batch_size, seq_q, seq_k]。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward中的CHECK不明白是在干什么？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### stackedCell = onmt.modules.stackedLSTM\n",
    "看了stackedLSTM，实在是没有看出区别在哪里，难道nn.LISTM实现的时候，除了第一层都是不加input的吗？？只能这么理解了。没事还是多翻翻源代码吧？？？input_feeding应该是指用额外的向量来表达已经翻译够的单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'opt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3339893780b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 类什么的，也是可以这么玩的。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"LSTM\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mstackedCell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStackedLSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstackedCell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStackedGRU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'opt' is not defined"
     ]
    }
   ],
   "source": [
    "# 类什么的，也是可以这么玩的。\n",
    "if opt.rnn_type == \"LSTM\":\n",
    "    stackedCell = onmt.modules.StackedLSTM\n",
    "else:\n",
    "    stackedCell = onmt.modules.StackedGRU\n",
    "self.rnn = stackedCell(opt.layers, input_size, opt.rnn_size, opt.dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ContextGateFactory\n",
    "工厂模式啊，好久没见过了，是这么弄的吗？？？这里主要用来调整attention和decoder state对当前循环的影响的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ContextGateFacctory(type, embeddings_size, decoder_size, attention_size, output_size):\n",
    "    gate_types = {'source': SourceContextGate,\n",
    "                 'target': TargetContextGate,\n",
    "                 'both': BothContextGate}\n",
    "    assert type in gate_types, \"not valid contextGate type: {0}\".format(type)\n",
    "    return gate_type[type](embeddings_size, decoder_size, attention_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attention\n",
    "- view(*args) → Tensor\n",
    "\n",
    "Returns a new tensor with the same data but different size.<br>\n",
    "The returned tensor shares the same data and must have the same number of elements, but may have a different size. A tensor must be contiguous() to be viewed.\n",
    "- expand_as(tensor)<br>\n",
    "\n",
    "Expands this tensor to the size of the specified tensor.\n",
    "- contiguous() → Tensor\n",
    "\n",
    "Returns a contiguous Tensor containing the same data as this tensor. If this tensor is contiguous, this function returns the original tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "#### pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
    "seq[start:end:step]\n",
    "#### make_positional_encoding(self, dim, max_len):\n",
    "可以看懂代码，但是不知道为什么要这么弄？？\n",
    "#### if opt.feat_merge == 'concat': emb_sizes.extend([int(feat_dict.size() ** feat_exp) for feat_dict in feature_dicts])\n",
    "为什么要弄成一样？？？不了解他的好处在哪里？？？\n",
    "#### features = [lut(feat) for lut, feat in zip(self.emb_luts, feat_inputs)]\n",
    "还是摆弄方法名，觉得有意思，就又写了一次。\n",
    "#### emb = self.dropout(emb)\n",
    "为什么在这里加dropout，其实不知道dropout一般在哪里加？？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNDecoderState(DecoderState)\n",
    "#### if not isinstance(rnnsatete, tuple): self.hidden = (rnnstate,) else: self.hidden = rnnstate\n",
    "对tupel不是很了解？？？所以并不理解其中的区别？？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train.py\n",
    "#### nn.Moudle.state_dict(destination=None, prefix='')\n",
    "Returns a dictionary containing a whole state of the module.<br>\n",
    "Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.\n",
    "#### torch.save(obj, f, pickle_module=$<$module 'pickle' from '/home/jenkins/miniconda/lib/python3.5/pickle.py'$>$, pickle_protocol=2)\n",
    "Saves an object to a disk file.<br>\n",
    "Parameters:\t<br>\n",
    "obj – saved object<br>\n",
    "f – a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name<br>\n",
    "pickle_module – module used for pickling metadata and objects<br>\n",
    "pickle_protocol – can be specified to override the default protocol<br>\n",
    "\n",
    "#### torch.load(f, map_location=None, pickle_module=$<$module 'pickle' from '/home/jenkins/miniconda/lib/python3.5/pickle.py'$>$)\n",
    "Loads an object saved with torch.save() from a file.<br>\n",
    "torch.load can dynamically remap storages to be loaded on a different device using the map_location argument. If it’s a callable, it will be called with two arguments: storage and location tag. It’s expected to either return a storage that’s been moved to a different location, or None (and the location will be resolved using the default method). If this argument is a dict it’s expected to be a mapping from location tags used in a file, to location tags of the current system.<br>\n",
    "By default the location tags are ‘cpu’ for host tensors and ‘cuda:device_id’ (e.g. ‘cuda:2’) for cuda tensors. User extensions can register their own tagging and deserialization methods using register_package.<br>\n",
    "Parameters:\t<br>\n",
    "f – a file-like object (has to implement fileno that returns a file descriptor, and must implement seek), or a string containing a file name<br>\n",
    "map_location – a function or a dict specifying how to remap storage locations<br>\n",
    "pickle_module – module used for unpickling metadata and objects (has to match the pickle_module used to serialize file)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 明天的大概的任务\n",
    "- 卡在checkpoint了，先跳转一下\n",
    "- 继续看openNMT的代码\n",
    "- 看完后，总结画个草图。\n",
    "- 回头把readPaper中的图片给加上\n",
    "- positional embedding的问题还没有搞清楚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
