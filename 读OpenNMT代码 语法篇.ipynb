{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本上没有什么顺序可言。模型都是比较熟悉的。但因为使用PyTorch，所以很多语法都不熟悉，所以就是把觉得比较新奇的语句给摘出来。<br>\n",
    "<br>\n",
    "一直搞不懂的是，为什么这些类明明没有实现__call__方法，但是都可以直接调用，而且好像默认调用了其中的forward方法。是因为nn.Modul中实现了__call__方法，而其中就有调用forward这个语句吗？？？有时间应该去看看源码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### argparse.ArgumentParser\n",
    "是python的一个命令行解析包。相当于tensorflow中的FLAGS吧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-v VERBOSITY] [-v2] [-x X] [-v3 {0,1,2}]\n",
      "                             echo\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/py3-tf/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"echo\") # 定义一个叫echo的参数，默认必选\n",
    "# 定义了可选参数-v或--verbosity，通过解析后，其值保存在args.verbosity变量中\n",
    "parser.add_argument(\"-v\", \"--verbosity\", help = \"increase output verbosity\")\n",
    "# 默认为True，不出现则为False\n",
    "parser.add_argument(\"-v2\", \"--verbosity2\", help = \"increase output verbosity\", action = \"store_true\")\n",
    "# argparse提供了对参数类型的解析，如果类型不符合，则直接报错\n",
    "parser.add_argument(\"-x\", type = int, help = \"the base\")\n",
    "# 可以设置可选值\n",
    "parser.add_argument(\"-v3\", \"--verbosity3\", type = int, choices = [0, 1, 2], help = \"increase output verbosity\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "if args.verbosity3 == 2:\n",
    "    print(\"th\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "主要确定：\n",
    "- 是否bidirection\n",
    "- 层的数量\n",
    "- 每层节点的数量\n",
    "- 使用什么层结果RNN，LSTM还是Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### assert opt.rnn_size % self.num_directions == 0\n",
    "使用assert断言是学习python一个非常好的习惯，python assert 断言句语格式及用法很简单。在没完善一个程序之前，我们不知道程序在哪里会出错，与其让它在运行最崩溃，不如在出现错误条件时就崩溃，这时候就需要assert断言的帮助。<br>\n",
    "python assert断言是声明其布尔值必须为真的判定，如果发生异常就说明表达示为假。可以理解assert断言语句为raise-if-not，用来测试表示式，其返回值为假，就会触发异常。<br>\n",
    "assert的异常参数，其实就是在断言表达式后添加字符串信息，用来解释断言并更好的知道是哪里出了问题。格式如下：<br>\n",
    "assert expression [, arguments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "坑爹啊",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6022c7154b0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"坑爹啊\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: 坑爹啊"
     ]
    }
   ],
   "source": [
    "assert 1==2, \"坑爹啊\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### self.hidden_size = opt.rnn_size // self.num_directions\n",
    "opt.rnn_size: size of LSTM hidden states<br>\n",
    "python 2.x里面，// 是地板除，/如果有一个数是浮点数就得到小数，如果两个都是整数也是地板除。<br>\n",
    "python 3.x里面，// 是地板除，/ 不管两边是不是整数得到的都是小数。<br>\n",
    "四舍五入请用: round(5/3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1.6666666666666667, 2, 1.67)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 5 // 3 \n",
    "b = 5 / 3\n",
    "c = round(5/3)\n",
    "d = round(5/3, 2)\n",
    "a, b, c, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.ModuleList(Modules = None)\n",
    "nn.ModuleList(<br>\n",
    "&emsp;[onmt.modules.TransformerEncoder(self.hidden_size, opt)<br>\n",
    "&emsp;for i in range(opt.layers)])<br>\n",
    "transformerEncoder实现每层由两部分组成：self-attention层和positionwiseFeedForward（bottleLinear? + ReLu + dropout + residual）层<br>\n",
    "Holds submodules in a list.<br>\n",
    "ModuleList can be indexed like a regular Python list, but modules it contains are properly registered, and will be visible by all Module methods.<br>\n",
    "Parameters:\tmodules (list, optional) – a list of modules to add<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### self.rnn = getattr(nn, opt.run_type)(...)\n",
    "getattr(object, name[, default])<br>\n",
    "Return the value of the named attribute of object. name must be a string. If the string is the name of one of the object’s attributes, the result is the value of that attribute. For example, getattr(x, 'foobar') is equivalent to x.foobar. If the named attribute does not exist, default is returned if provided, otherwise AttributeError is raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-ed73608e3478>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-ed73608e3478>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    if self.encoder_layer = \"transformer\":\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 这样搭网络感觉牛牛哒。 另外nn.Module还是很陌生，\n",
    "if self.encoder_layer = \"transformer\":\n",
    "    self.transformer = nn.ModuleList(\n",
    "        [onmt.modules.TransformerEncoder(self.hidden_size, opt)\n",
    "         for i in range(opt.layers)])\n",
    "else:\n",
    "    self.rnn = getattr(nn, opt.rnn_type)(\n",
    "        input_size, self.hidden_size, num_layers = out.layers,\n",
    "        dropout = out.dropout,\n",
    "        bidirectional = opt.brnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pad_attn_mask = seq_k.data.eq(onmt.Constants.PAD).unsequeeze(1).expand(mb_size, len_q, len_k)\n",
    "- torch.eq(input, other, out=None) → Tensor: <br>\n",
    "\n",
    "Computes element-wise equality. The second argument can be a number or a tensor whose shape is broadcastable with the first argument.<br>\n",
    "return: a torch.ByteTensor containing a 1 at each location where the tensors are equal and a 0 at every other location<br>\n",
    "- torch.unsqueeze(input, dim, out=None)<br>\n",
    "\n",
    "Returns a new tensor with a dimension of size one inserted at the specified position. The returned tensor shares the same underlying data with this tensor.<br>\n",
    "A negative dim value can be used and will correspond to dim+input.dim()+1<br>\n",
    "- expand(tensor, sizes) → Tensor<br>\n",
    "\n",
    "Returns a new view of the tensor with singleton dimensions expanded to a larger size.<br>\n",
    "Tensor can be also expanded to a larger number of dimensions, and the new ones will be appended at the front.<br>\n",
    "Expanding a tensor does not allocate new memory, but only creates a new view on the existing tensor where a dimension of size one is expanded to a larger size by setting the stride to 0. Any dimension of size 1 can be expanded to an arbitrary value without allocating new memory.<br>\n",
    "这是一个生成mask的代码。torch.eq找出向量中的特殊单词（Constants.PAD），unsequeeze是增加一个维，该维的长度为1，expand扩展（复制）该mask的维度扩展成给定的维度。因为这个mask是给matmal(Q,K)用的，所以其维度[batch_size, seq_q, seq_k]。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward中的CHECK不明白是在干什么？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### stackedCell = onmt.modules.stackedLSTM\n",
    "看了stackedLSTM，实在是没有看出区别在哪里，难道nn.LISTM实现的时候，除了第一层都是不加input的吗？？只能这么理解了。没事还是多翻翻源代码吧？？？input_feeding应该是指用额外的向量来表达已经翻译够的单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'opt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3339893780b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 类什么的，也是可以这么玩的。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"LSTM\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mstackedCell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStackedLSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstackedCell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStackedGRU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'opt' is not defined"
     ]
    }
   ],
   "source": [
    "# 类什么的，也是可以这么玩的。\n",
    "if opt.rnn_type == \"LSTM\":\n",
    "    stackedCell = onmt.modules.StackedLSTM\n",
    "else:\n",
    "    stackedCell = onmt.modules.StackedGRU\n",
    "self.rnn = stackedCell(opt.layers, input_size, opt.rnn_size, opt.dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ContextGateFactory\n",
    "工厂模式啊，好久没见过了，是这么弄的吗？？？这里主要用来调整attention和decoder state对当前循环的影响的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ContextGateFacctory(type, embeddings_size, decoder_size, attention_size, output_size):\n",
    "    gate_types = {'source': SourceContextGate,\n",
    "                 'target': TargetContextGate,\n",
    "                 'both': BothContextGate}\n",
    "    assert type in gate_types, \"not valid contextGate type: {0}\".format(type)\n",
    "    return gate_type[type](embeddings_size, decoder_size, attention_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attention\n",
    "- view(*args) → Tensor\n",
    "\n",
    "Returns a new tensor with the same data but different size.<br>\n",
    "The returned tensor shares the same data and must have the same number of elements, but may have a different size. A tensor must be contiguous() to be viewed.\n",
    "- expand_as(tensor)<br>\n",
    "\n",
    "Expands this tensor to the size of the specified tensor.\n",
    "- contiguous() → Tensor\n",
    "\n",
    "Returns a contiguous Tensor containing the same data as this tensor. If this tensor is contiguous, this function returns the original tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "#### pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
    "seq[start:end:step]\n",
    "#### make_positional_encoding(self, dim, max_len):\n",
    "可以看懂代码，但是不知道为什么要这么弄？？\n",
    "#### if opt.feat_merge == 'concat': emb_sizes.extend([int(feat_dict.size() ** feat_exp) for feat_dict in feature_dicts])\n",
    "为什么要弄成一样？？？不了解他的好处在哪里？？？\n",
    "#### features = [lut(feat) for lut, feat in zip(self.emb_luts, feat_inputs)]\n",
    "还是摆弄方法名，觉得有意思，就又写了一次。\n",
    "#### emb = self.dropout(emb)\n",
    "为什么在这里加dropout，其实不知道dropout一般在哪里加？？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNDecoderState(DecoderState)\n",
    "#### if not isinstance(rnnsatete, tuple): self.hidden = (rnnstate,) else: self.hidden = rnnstate\n",
    "对tupel不是很了解？？？所以并不理解其中的区别？？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make_base_model\n",
    "#### class torch.nn.Sequential(*args)[source]\n",
    "A sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an ordered dict of modules can also be passed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 例子\n",
    "# Example of using Sequential\n",
    "model = nn.Sequential(\n",
    "          nn.Conv2d(1,20,5),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(20,64,5),\n",
    "          nn.ReLU()\n",
    "        )\n",
    "\n",
    "# Example of using Sequential with OrderedDict\n",
    "model = nn.Sequential(OrderedDict([\n",
    "          ('conv1', nn.Conv2d(1,20,5)),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('conv2', nn.Conv2d(20,64,5)),\n",
    "          ('relu2', nn.ReLU())\n",
    "        ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train.py\n",
    "#### dill\n",
    "dill extends python’s pickle module for serializing and de-serializing python objects to the majority of the built-in python types. Serialization is the process of converting an object to a byte stream, and the inverse of which is converting a byte stream back to on python object hierarchy.<br>\n",
    "dill provides the user the same interface as the pickle module, and also includes some additional features. <br>\n",
    "dill can be used to store python objects to a file, but the primary usage is to send python objects across the network as a byte stream.<br>\n",
    "\n",
    "#### opts\n",
    "opts is a simple python library which allows you to easiely parse command line arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'opts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4e2d74299118>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mopts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBooleanOption\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m parser = Parser(description=u\"Our own awesome dvcs\", commands={\n\u001b[1;32m      4\u001b[0m     \"add\": Command(\n\u001b[1;32m      5\u001b[0m         \u001b[0mshort_description\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu\"Adds a file to the repository\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'opts'"
     ]
    }
   ],
   "source": [
    "from opts import Parser, Command, Option, BooleanOption\n",
    "\n",
    "parser = Parser(description=u\"Our own awesome dvcs\", commands={\n",
    "    \"add\": Command(\n",
    "        short_description=u\"Adds a file to the repository\",\n",
    "        options={\n",
    "            \"dry-run\": BooleanOption(\"n\", \"dry-run\"),\n",
    "            \"interactive\": BooleanOption(\"i\", \"interactive\"),\n",
    "        },\n",
    "    ),\n",
    "    \"stack\": Command(),\n",
    "    \"stash\": Command(),\n",
    "    \"filename\": Option(\"f\", \"--file\",\n",
    "        short_description=u\"write report to this file\")\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### opts.add_md_help_argument(parser), opts.model_opts(parser), opts.train_opts(parser)\n",
    "这几个方法没有找到，另外都用了argparse了，为什么还要用opts呢，他的好处体现在哪里？？？\n",
    "好像误会了，这里的opts并不是网上的包，而是他们自己写的一个python文件。里面包含了他预设的几个参数，包括model_opts核train_opts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.Moudle.state_dict(destination=None, prefix='')\n",
    "Returns a dictionary containing a whole state of the module.<br>\n",
    "Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.<br>\n",
    "model_state_dict = (model.module.state_dict() if len(opt.gpus) > 1 else model.state_dict())\n",
    "#### torch.save(obj, f, pickle_module=$<$module 'pickle' from '/home/jenkins/miniconda/lib/python3.5/pickle.py'$>$, pickle_protocol=2)\n",
    "Saves an object to a disk file.<br>\n",
    "Parameters:\t<br>\n",
    "obj – saved object<br>\n",
    "f – a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name<br>\n",
    "pickle_module – module used for pickling metadata and objects<br>\n",
    "pickle_protocol – can be specified to override the default protocol<br>\n",
    "\n",
    "#### torch.load(f, map_location=None, pickle_module=$<$module 'pickle' from '/home/jenkins/miniconda/lib/python3.5/pickle.py'$>$)\n",
    "Loads an object saved with torch.save() from a file.<br>\n",
    "torch.load can dynamically remap storages to be loaded on a different device using the map_location argument. If it’s a callable, it will be called with two arguments: storage and location tag. It’s expected to either return a storage that’s been moved to a different location, or None (and the location will be resolved using the default method). If this argument is a dict it’s expected to be a mapping from location tags used in a file, to location tags of the current system.<br>\n",
    "By default the location tags are ‘cpu’ for host tensors and ‘cuda:device_id’ (e.g. ‘cuda:2’) for cuda tensors. User extensions can register their own tagging and deserialization methods using register_package.<br>\n",
    "Parameters:\t<br>\n",
    "f – a file-like object (has to implement fileno that returns a file descriptor, and must implement seek), or a string containing a file name<br>\n",
    "map_location – a function or a dict specifying how to remap storage locations<br>\n",
    "pickle_module – module used for unpickling metadata and objects (has to match the pickle_module used to serialize file)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> torch.load('tensors.pt')\n",
    "# Load all tensors onto the CPU\n",
    ">>> torch.load('tensors.pt', map_location=lambda storage, loc: storage)\n",
    "# Map tensors from GPU 1 to GPU 0\n",
    ">>> torch.load('tensors.pt', map_location={'cuda:1':'cuda:0'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.manual_seed(seed)[source]\n",
    "Sets the seed for generating random numbers. And returns a torch._C.Generator object.\n",
    "#### cuda.set_device(opt.gpus[0])\n",
    "Sets the current device.<br>\n",
    "Usage of this function is discouraged in favor of device. In most cases it’s better to use CUDA_VISIBLE_DEVICES environmental variable.<br>\n",
    "- torch.cuda\n",
    "\n",
    "This package adds support for CUDA tensor types, that implement the same function as CPU tensors, but they utilize GPUs for computation.\n",
    "It is lazily initialized, so you can always import it, and use is_available() to determine if your system supports CUDA.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cc = CrayonClient(hostname=opt.log_server)\n",
    "- get_experiment_names()\n",
    "\n",
    "Returns a list of string containing the name of all the experiments on the server.<br>\n",
    "- create_experiment(xp_name, zip_file=None)\n",
    "\n",
    "Creates a new experiment with name xp_name and returns a CrayonExperiment object.\n",
    "If zip_file is provided, this experiment is initialized with the content of the zip file (see CrayonExperiment.to_zip to get the zip file)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### onmt.Loss.MemoryEfficientLoss\n",
    "？？？？\n",
    "\n",
    "#### opt.extra_shuffle and epoch > opt.curriculum:\n",
    "opt.extra_shuffle也没找到这个变量。opt.curriculum不知道是干什么的？？？\n",
    "\n",
    "#### torch.randperm(n, out=None) → LongTensor\n",
    "Returns a random permutation of integers from 0 to n - 1.\n",
    "\n",
    "#### opt.truncated_decoder if opt.truncated_decoder\n",
    "Truncated BPTT is an approximation of full BPTT that is preferred for long sequences, since full BPTT’s forward/backward cost per parameter update becomes very high over many time steps. The downside is that the gradient can only flow back so far due to that truncation, so the network can’t learn dependencies that are as long as in full BPTT.\n",
    "\n",
    "#### if dec_state is not None: dec_state.detach()\n",
    "？？？？\n",
    "\n",
    "#### model_state_dict = {k: v for k, v in model_state_dict.items() if 'generator' not in k}\n",
    "为什么要去除含generator的？？？\n",
    "\n",
    "#### os.path.abspath(path)\n",
    "获得绝对路径\n",
    "Return a normalized absolutized version of the pathname path. On most platforms, this is equivalent to calling the function normpath()\n",
    "\n",
    "#### dataset = torch.load(opt.data) 333\n",
    "不是只能用来读取参数的啊？？还是这里只是读了框架\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)[source]\n",
    "Implements data parallelism at the module level.<br>\n",
    "This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension. In the forward pass, the module is replicated on each device, and each replica handles a portion of the input. During the backwards pass, gradients from each replica are summed into the original module.<br>\n",
    "The batch size should be larger than the number of GPUs used. It should also be an integer multiple of the number of GPUs so that each chunk is the same size (so that each GPU processes the same number of samples).<br>\n",
    "Parameters:\t<br>\n",
    "module – module to be parallelized<br>\n",
    "device_ids – CUDA devices (default: all devices)<br>\n",
    "output_device – device location of output (default: device_ids[0])<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## onmt.Loss.py\n",
    "#### nn.NLLLoss(weight, size_average=False)\n",
    "class torch.nn.NLLLoss(weight=None, size_average=True, ignore_index=-100)[source]<br>\n",
    "The negative log likelihood loss. It is useful to train a classification problem with n classes<br>\n",
    "If provided, the optional argument weights should be a 1D Tensor assigning weight to each of the classes.<br>\n",
    "This is particularly useful when you have an unbalanced training set.<br>\n",
    "The input given through a forward call is expected to contain log-probabilities of each class: input has to be a 2D Tensor of size (minibatch, n)<br>\n",
    "Obtaining log-probabilities in a neural network is easily achieved by adding a LogSoftmax layer in the last layer of your network.<br>\n",
    "You may use CrossEntropyLoss instead, if you prefer not to add an extra layer.<br>\n",
    "The target that this loss expects is a class index (0 to N-1, where N = number of classes)<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CopyGenerator.py\n",
    "### torch.nn.functional\n",
    "包括：Convolution functions，Pooling functions，Non-linear activation functions，Normalization functions，Linear functions，Dropout functions，Distance functions，Loss functions，Vision functions\n",
    "### torch.max(input, dim, keepdim=False, out=None) -> (Tensor, LongTensor)\n",
    "Returns the maximum value of each row of the input Tensor in the given dimension dim. The second return value is the index location of each maximum value found (argmax).\n",
    "### torch.sort(input, dim=None, descending=False, out=None) -> (Tensor, LongTensor)\n",
    "Sorts the elements of the input Tensor along a given dimension in ascending order by value.<br>\n",
    "If dim is not given, the last dimension of the input is chosen.<br>\n",
    "If descending is True then the elements are sorted in descending order by value.<br>\n",
    "A tuple of (sorted_tensor, sorted_indices) is returned, where the sorted_indices are the indices of the elements in the original input Tensor.<br>\n",
    "### cpu()\n",
    "Returns a CPU copy of this tensor if it’s not already on the CPU\n",
    "### _, ids = dttn[0].cpu().data.sort(0, descending = True) L53\n",
    "这个式子中的cpu不知道是为了做什么？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## onmt.Optim.py\n",
    "#### torch.optim\n",
    "包含多个优化器，构建的时候使用要调整的变量及学习率等作为参数，调用时，使用step方法<br>\n",
    "for input, target in dataset:<br>\n",
    "&nbsp;&nbsp; optimizer.zero_grad()<br>\n",
    "&nbsp;&nbsp; output = model(input)<br>\n",
    "&nbsp;&nbsp; loss = loss_fn(output, target)<br>\n",
    "&nbsp;&nbsp; loss.backward()<br>\n",
    "&nbsp;&nbsp; optimizer.step()<br>\n",
    "#### self.optimizer.param_groups[0]['lr'] = self.lr\n",
    "optimizer里面没有param_groups啊？？？\n",
    "#### torch.nn.utils.clip_grad_norm(parameters, max_norm, norm_type=2)[source]\n",
    "Clips gradient norm of an iterable of parameters.<br>\n",
    "The norm is computed over all gradients together, as if they were concatenated into a single vector. Gradients are modified in-place.<br>\n",
    "Returns: Total norm of the parameters (viewed as a single vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## onmt.IO.py\n",
    "#### torchtext.data, torchtext.vocab\n",
    "木找到相关的内容，？？？？\n",
    "#### from collections import Counter\n",
    "我们都知道，Python拥有一些内置的数据类型，比如str, int, list, tuple, dict等， collections模块在这些内置数据类型的基础上，提供了几个额外的数据类型：<br>\n",
    "- namedtuple(): 生成可以使用名字来访问元素内容的tuple子类\n",
    "- deque: 双端队列，可以快速的从另外一侧追加和推出对象\n",
    "- Counter: 计数器，主要用来计数\n",
    "- OrderedDict: 有序字典\n",
    "- defaultdict: 带有默认值的字典\n",
    "- namedtuple()\n",
    "\n",
    "namedtuple主要用来产生可以使用名称来访问元素的数据对象，通常用来增强代码的可读性， 在访问一些tuple类型的数据时尤其好用。\n",
    "#### torchtext.vocab.Vocab(merged, specials = [PAD_WORD, BOS_WORD, EOS_WORD], max_size = vocab_size)\n",
    "还是torchtext？？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 后面的大概的任务\n",
    "- 得去看一下torchtext了， 汗一个\n",
    "- 继续看openNMT的代码\n",
    "- 看完后，总结画个草图。\n",
    "- 回头把readPaper中的图片给加上\n",
    "- positional embedding的问题还没有搞清楚\n",
    "- stats = onmt.Loss.Statistics()\n",
    "- onmt.Loss.MemoryEfficientLoss\n",
    "- nn.Module的train mode和eval mode具体的区别在哪里？\n",
    "- onmt.Loss.NMTCriterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2-tf",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
