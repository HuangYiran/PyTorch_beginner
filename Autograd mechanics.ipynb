{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excluding subgraphs from backward\n",
    "每个变量(Variable)都有两个标签：requires_grad和volatile。通过这两个标签中的任何一个，我们都可以控制，在后向梯度运算的时候，部分子图不会被计算到，从而提高整体运算的效率。\n",
    "#### reguires_grad\n",
    "假设存在一个运算Operation，那么他的输出使不需要梯度的，当且仅当该运算的所有输入都是不需要梯度的。当一个子图中的所有变量都是不需要梯度的，那么这个子图在后向传播计算梯度的时候，不会被计算到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x = Variable(torch.rand(5, 5))\n",
    "y = Variable(torch.rand(5, 5))\n",
    "z = Variable(torch.rand(5, 5), requires_grad = True)\n",
    "a = x + y\n",
    "print(a.requires_grad)\n",
    "b = a + z\n",
    "print(b.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种方法在想冻结部分模型的时候相当有用<br>\n",
    "For example if you want to finetune a pretrained CNN, it's enouhh to switch the requires_grad flags in the frozen base, and no intermediate buffers will be saved, until the computations gets to the last layer, where the affine transform will use weights that require gradient, and the output of the network will also require them.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "model = resnet18(pretrained = True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "# Replace the last fully connected layer\n",
    "# Parameters of newly constructed modules have requires_grad = True by default\n",
    "model.fc = nn.Linear(512, 100)\n",
    "# Optimize only the classifier\n",
    "optimizer = optim.SGD(model.fc.parameters(), lr = 1e-2, momentum = 0.9)\n",
    "# 不是都关了吗，把整个model的参数都放进去还有差别吗？？？？？？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### volatile\n",
    "Volatile一般推荐在推理模式中(inference model)使用，也就是说当你很确定你不会用.backward()的时候。他比其他的autograd设置，更加有效。同时volatile也意味着require_grad的值是False。\n",
    "Volatile和require_grad的区别主要在于他们传播的方式。假设我们有一个运算Operation，那么当其中存在一个input是Volatile的时候，对应的输出也就是Volatile的。Volatile传播得更快，因为对于一个子图，你只需要设置他起初的一个input为Volatile的就行了，而使用require_grad，你需要把其中每个变量都设一遍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require_input = Variable(torch.randn(1, 3, 227, 227))\n",
    "volatile_input = Variable(torch.randn(1, 3, 227, 227), volatile = True)\n",
    "model = resnet18(pretrained = True)\n",
    "model(require_input).requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(volatile_input).requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(volatile_input).volatile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(volatile_input).grad_fn is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How autograd encoded the history\n",
    "Autograd is reverse automatic differentiation system.不知道怎么翻译？？？<br>\n",
    "从概念上来讲，autograd记录着一个无环图，在这个图中记载着用于产生对应输出的所有运算过程，他的叶子节点代表输入变量，根节点代表输出。通过链式规则，我们由根节点到各个叶子节点算出其中各个节点的梯度。<br>\n",
    "从内部实现来说，Autograd描述的是一个由function组成的无向图。(which can be apply()-ed to cimpute the result of evaluating the graph). 当执行Forward运算的时候，autograd会同时处理要求的所有计算，并构建一个用于计算梯度的无向图。当前向传播搞定之后，他通过这个新构建的图，计算各个节点的梯度。<br>\n",
    "一个比较重要的点是，在循环中的每次运算都会重新生成一个图。你完全可以在这期间，通过控制流，改变图的结构和大小。也就是说，每次循环跑的图，可以是不一样的。同样的，你也不需要等到实现了所有可能的路径之后，再进行训练。<br>\n",
    "An important thing to note is that the graph is recreated from scratch at evary iteration, and this is exactly what allows for using arbitrary Python control flow statements, that can change  the overall shape and size of the graph at every iteration. You don't have to encode all possible paths before you launch the training - what you run is what you differentialte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-place operations on Variables\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
