{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类Encoder(nn.Module):\n",
    "### 变量\n",
    "- layers: rnn的层数\n",
    "- num_directions: 整数类型，指明是否双向\n",
    "- hidden_size: 每层cnn，单向隐藏层神经元个数 = opt.rnn_size // self.num_directions\n",
    "- input_size: word_vec_size\n",
    "- word_lut: nn.Embedding, \n",
    "    - Input: LongTensor (N, W), N = seq_len, W = batch_size\n",
    "    - Output: (N, W, embedding_dim)\n",
    "    - 实际上就是把matrix中的东西给转换了，并没有固定的说那个维度代表的是什么意义\n",
    "- rnn: nn.LSTM\n",
    "    - Inputs: input, (h_0, c_0)\n",
    "        - input (seq_len, batch, input_size)\n",
    "        - h_0 (num_layers * num_directions, batch, hidden_size)\n",
    "        - c_0 (num_layers * num_directions, batch, hidden_size)\n",
    "    - Outputs: output, (h_n, c_n)\n",
    "        - output (seq_len, batch, hidden_size * num_directions)\n",
    "        - h_n (num_layers * num_directions, batch, hidden_size)\n",
    "        - c_n (num_layers * num_directions, batch, hidden_size)\n",
    "\n",
    "### 方法\n",
    "##### __init__(self, opt, dicts):\n",
    "- 初始化变量，基本上就是上面几个了。\n",
    "\n",
    "##### load_pretrained_vectors(self, opt):\n",
    "- 如果有训练过的embedding模型，可以直接进行读取\n",
    "- self.word_lut.weight.data.copy_(torch.load.(opt.pre_word_vecs_dec))\n",
    "\n",
    "##### forward(self, input, hidden = None):\n",
    "- Inputs: input, (h_0, c_0)\n",
    "    - input (seq_len, batch)\n",
    "- Output: (h_n, c_n), output\n",
    "- 在这个OpenNMT中，输入input一般还包括了lengths信息。如果是的话，那么我们先用pack_padded_sequence方法，把emb后的数据和lengths组合一起生成对应的PackedSequence对象emb。好处暂时不明\n",
    "    - pack(self.word_lut(input[0]), lengths)\n",
    "- 把emb扔进rnn中进行运算得到对应的输出outputs, hidden_t\n",
    "- 如果输入是packedSegmentation，那么这里要对结果进行unpack: pad_packed_sequence\n",
    "    - outputs = unpack(outputs)[0]\n",
    "- 注意这里输出的顺序并不和rnn的输出的顺序一样\n",
    "    - return hiddent_t, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类StackedLSTM(nn.Module):\n",
    "主要是为了辅助decoder用的, 主要是为了在前后两个单词的运算之间加入Attention的信息，所以必须自己写一个，在下面可以看到是怎么回事\n",
    "### 变量\n",
    "- dropout: nn.Dropout(dropout)\n",
    "- num_layers: 隐藏层的层数\n",
    "- layers: nn.ModuleList() 定义各个层的神经元类型，这里都是nn.LSTMCell(input_size, rnn_size)\n",
    "    - Inputs: input, (h_0, c_0)\n",
    "        - input (batch, input_size)\n",
    "        - h_0 (batch, hidden_size)\n",
    "        - c_0 (batch, hidden_size)\n",
    "    - Outputs: h_1, c_1\n",
    "        - h_1 (batch, hidden_size)\n",
    "        - c_1 (batch, hidden_size)\n",
    "\n",
    "### 方法\n",
    "##### __init__(self, input, hidden)\n",
    "- 初始化上面提到的变量，第一层的input和hidden的size可能不一样，因为在decode中可能会有合并上一层的输出等操作\n",
    "\n",
    "##### forward(self, input, hidden)\n",
    "- Inputs: input, (h_0, c_0)\n",
    "    - input (batch, input_size)\n",
    "    - h_0 (num_layers, batch, hidden_size)\n",
    "    - c_0 (num_layers, batch, hidden_size)\n",
    "- Outputs: output, (h_1, c_1)\n",
    "    - output (batch, hidden_size)\n",
    "    - h_1 (num_layers, batch, hidden_size)\n",
    "    - c_1 (num_layers, batch, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类Decoder(nn.Module):\n",
    "### 变量\n",
    "- layers: 隐藏层的层数\n",
    "- input_feed: 指明是仅使用input数据还是要结合其他的数据(比如上一个时间点的隐藏层的输出)作为input\n",
    "- input_size: word_vec_size, 如果input_feed是True的话，input_size = word_vec_size + rnn_size\n",
    "- word_lut: nn.Embedding\n",
    "    - Input: LongTensor (N, W), N = batch_size, W = seq_len\n",
    "    - Output: (N, W, embedding_dim)\n",
    "- rnn: StackedLSTM(layers, input_size, rnn_size, dropout)\n",
    "    - Inputs: input, (h_0, c_0)\n",
    "        - input (batch, input_size)\n",
    "        - h_0 (num_layers, batch, hidden_size)\n",
    "        - c_0 (num_layers, batch, hidden_size)\n",
    "    - Outputs: output, (h_1, c_1)\n",
    "        - output (batch, hidden_size)\n",
    "        - h_1 (num_layers, batch, hidden_size)\n",
    "        - c_1 (num_layers, batch, hidden_size)\n",
    "- attn: onmt.modules.GlobalAttention(rnn_size)\n",
    "    - Inputs: input, context\n",
    "        - input (batch, hidden_size) 代表Query\n",
    "        - context (batch, seq_len, hidden_size * num_directions) 代表key和value\n",
    "            -???双向怎么办？这里好像没有考虑这种情况，至少没在这里进行考虑\n",
    "    - Outputs: output, attn\n",
    "        - output (batch, rnn_size)\n",
    "        - attn (batch, seq_len)\n",
    "- dropout: nn.Dropout\n",
    "- hidden_size: 隐藏层每层神经元的数量rnn_size\n",
    "\n",
    "### 方法\n",
    "##### __init__(self, opt, dicts)\n",
    "- 初始化上面提到的变量\n",
    "\n",
    "##### load_pretrained_vectors(self, opt):\n",
    "- 如果有训练好的emb，那么就直接读取了\n",
    "- self.word_lut.weight.data.copy_(torch.load.(opt.pre_word_vecs_dec))\n",
    "\n",
    "##### forward(self, input , hidden, context, init_output)\n",
    "- Inputs: input, (h_0, c_0), context, init_output\n",
    "    - input (seq_len, batch)\n",
    "    - h_0 (num_layers, batch, hidden_size)\n",
    "    - c_0 (num_layers, batch, hidden_size)\n",
    "    - context (seq_len, batch, hidden_size * num_directions)\n",
    "    - init_output (batch, embedding_dim)\n",
    "- Outputs: outputs, hidden, attn\n",
    "    - outputs (seq_len, batch, hidden_size)\n",
    "    - hidden (num_layers, batch, hidden_size) (h_n, c_n)\n",
    "    - attn (batch, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(self, input, hidden, context, init_output):\n",
    "    # input (seq_len, batch) \n",
    "    # emb (seq_len, batch, dim)\n",
    "    emb = self.word_lut(input)\n",
    "    \n",
    "    outputs = []\n",
    "    output = init_output\n",
    "    # torch.split能够把数据平分到指定大小的chunk中，这里的参数是指每个chunk的大小为1\n",
    "    # torch.split(tensor, split_size, dim=0)\n",
    "    # 这里是为了单独处理每一个单词，得到的emb_t的size为(1, batch, dim)\n",
    "    for emb_t in emb.split(1):\n",
    "        # 去掉数量为1的维度：emb_t (batch, dim)\n",
    "        emb_t =emb_t.squeeze(0)\n",
    "        # 如果设定了input_feed的话，就把emb_t和init_output链接起来，这样对应的input_size就变大了\n",
    "        if self.input_feed:\n",
    "            emb_t = torch.cat([emb_t, output], 1)\n",
    "        \n",
    "        # 放进stackedLSTM中，输出为 output, (h_1, c_1)\n",
    "        # - output (batch, hidden_size)\n",
    "        # - h_1 (num_layers, batch, hidden_size)\n",
    "        # - c_1 (num_layers, batch, hidden_size)\n",
    "        output, hidden = self.rnn(emb_t, hidden)\n",
    "        \n",
    "        # 使用output为key， context为value进行Attention操作\n",
    "        # context的size为(seq_len, batch, hidden_size * num_directions)\n",
    "        output, attn = self.attn(output, context.transpose(0, 1))\n",
    "        outputs += [output]\n",
    "    outpus = torch.stack(outputs)\n",
    "    return outputs, hidden, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类NMTModel(nn.Module)\n",
    "### 变量\n",
    "- encoder: encoder类\n",
    "- decoder: decoder类\n",
    "\n",
    "### 方法\n",
    "##### __init__(self, encoder, decoder)\n",
    "- 初始化\n",
    "\n",
    "##### make_init_decoder_output(self, context)\n",
    "- 建立decoder的初始output， 并赋值为0\n",
    "- Inputs: context\n",
    "    - context (seq_len, batch, hidden_size * num_directions)\n",
    "- Outputs: \n",
    "    - Variable(context.data.new(batch_size, decoder.hidden_size).zero_(), requires_grad = False)\n",
    "\n",
    "##### _fix_enc_hidden(self, h)\n",
    "- 维度转化，\n",
    "- Inputs: h\n",
    "    - h (num_layers * num_directions, batch, hidden_size)\n",
    "- Output: o\n",
    "    - o (num_layers, batch, directions * hidden_size)\n",
    "- 如果是双向的就进行以下转换：\n",
    "    - return h.view(h.size(0)//2, 2, h.size(1), h.size(2)).trainspose(1, 2).contiguous().view(h.size(0)//2, h.size(1), h.size(2) *2)\n",
    "\n",
    "##### forward(self, input):\n",
    "- 输入数据好像有点情况，目前还不能确定，所以就先放着吧？？？？？？？？\n",
    "\n",
    "##### tie_weights(self)\n",
    "- self.generator[0].weight = self.decoder.word_lut.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2-tf",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
