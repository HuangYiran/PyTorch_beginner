{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数\n",
    "- config: 指定配置文件\n",
    "- src_type: default = 'text' 指定输入的类型，还可以是img\n",
    "- src_img_dir: default = '.' 指定图片源\n",
    "- <font color=#0099ff>train_src: required = True 指定训练集的原文所在路径</font>\n",
    "- <font color=#0099ff>train_tgt: required = True 指定训练集的目标文所在路径</font>\n",
    "- <font color=#0099ff>valid_src: required = True</font>\n",
    "- <font color=#0099ff>valid_tgt: required = True</font>\n",
    "- <font color=#0099ff>save_data: required = True 指定预处理后的文件的存储位置</font>\n",
    "- src_vocab_size: type = int, default = 50000 指定原文词汇表的大小\n",
    "- tgt_vocab_size: type = int, default = 50000\n",
    "- src_vocab: 如果存在词汇表，指定原文词汇表所在位置\n",
    "- tgt_vocab: \n",
    "- src_seq_length: type = int, default = 50 指定原文最长的序列的长度\n",
    "- src_seq_length_trunc: type = int, default = 0 指定truncate的长度\n",
    "- tgt_seq_length: type = int, default = 50\n",
    "- tgt_seq_length_trunc: type = int, default = 0\n",
    "- shuffle: type = int, default = 1 shuffle data\n",
    "- seed: type = int, default = 3435 Random seed\n",
    "- lower: action = 'store_true' 使小写化\n",
    "- report_every: type = int, default = 100000 每处理指定数量的句子后，进行一次状态报告"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法\n",
    "##### makeVocabulary(filename, size):\n",
    "- 建立onmt.Dict对象vocab\n",
    "- 读文件，把每个单词加入到vocab中：vocab.add(word)\n",
    "- 根据size大小对词汇表进行剪枝: vocab = vocab.prune(size)\n",
    "- 返回vocab\n",
    "\n",
    "##### initVocabulary(name, dataFile, vocabFile, vocabSize):\n",
    "- 如果vocabFile不是none，则从这个文件中读取vocab: vocab.loadFile(vocabFile)\n",
    "- 如果vocabFile是none，那么就掉用前面提到的makeVocabulary方法，使用dataFile和vocabSize作为参数生成一个新的vocab\n",
    "- 返回vocab\n",
    "\n",
    "##### saveVocabulary(naem, vocab, file):\n",
    "- 保存词汇表: vocab.writeFile(file)\n",
    "\n",
    "#### makeData(srcFile, tgtFile, srcDicts, tgtDicts):\n",
    "- 注意1，这里包含空格的句子算是空句子，空句子将不会被处理\n",
    "- 注意2，单词数大于最大单词数seq_length的句子会被忽略\n",
    "- 读取原文文件，一次处理每个句子\n",
    "- 如果句子长度大于seq_length，则被忽略。否则，如果seq_length_trunc!=0，则被剪切到特定长度\n",
    "- 如果句子类型是text类型，则通过dict把每个单词转换为对应的编码，然后加入到src（列表）中\n",
    "- 如果句子类型为image类型，则通过torchvision的transforms.ToTensor方法转化为Tensor类型，然后加入到src中\n",
    "- 此外把每个需要处理的句子的长度存储到sizes列表中\n",
    "- 对目标文本进行类似的操作，不过最后存储到tgt列表中\n",
    "- 如果shuffle有设定，则通过torch.randperm对src，tgt进行乱序。\n",
    "- 通过_,perm = torch.sort(torch.Tensor(sizes))根据sizes的大小调整src和tgt中句子的排序(那上面的乱序还有意义吗？？？)\n",
    "- 输出操作信息\n",
    "- 返回src和tgt\n",
    "\n",
    "##### main()\n",
    "- 使用initVocabulary为原文和目标文生成词汇表，存储在词典类dicts中。其中项目src，tgt，分别指代原文和目标文的词汇表\n",
    "- 使用makeData，对原文和目标文做预处理(包括训练数据集和validation用的数据集)。这里用词典类存储处理后的文件：train = {}, valid = {}.每个词典中，分别有两个项目：src, tgt。分别对应着原文和目标文\n",
    "- 用一个词典类save_data，把上面生成的信息整合在一起。并使用torch.save把这些内容以序列的形式存储起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_data = {\n",
    "    'dicts': dicts,\n",
    "    'type': opt.src_type,\n",
    "    'train': train,\n",
    "    'valid': valid\n",
    "}\n",
    "troch.save(save_data, opt.save_data + '.train.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2-tf",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
