{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本上没有什么顺序可言。模型都是比较熟悉的。但因为使用PyTorch，所以很多语法都不熟悉，所以就是把觉得比较新奇的语句给摘出来。<br>\n",
    "<br>\n",
    "一直搞不懂的是，为什么这些类明明没有实现__call__方法，但是都可以直接调用，而且好像默认调用了其中的forward方法。是因为nn.Modul中实现了__call__方法，而其中就有调用forward这个语句吗？？？有时间应该去看看源码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### argparse.ArgumentParser\n",
    "是python的一个命令行解析包。相当于tensorflow中的FLAGS吧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-v VERBOSITY] [-v2] [-x X] [-v3 {0,1,2}]\n",
      "                             echo\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/py3-tf/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"echo\") # 定义一个叫echo的参数，默认必选\n",
    "# 定义了可选参数-v或--verbosity，通过解析后，其值保存在args.verbosity变量中\n",
    "parser.add_argument(\"-v\", \"--verbosity\", help = \"increase output verbosity\")\n",
    "# 默认为True，不出现则为False\n",
    "parser.add_argument(\"-v2\", \"--verbosity2\", help = \"increase output verbosity\", action = \"store_true\")\n",
    "# argparse提供了对参数类型的解析，如果类型不符合，则直接报错\n",
    "parser.add_argument(\"-x\", type = int, help = \"the base\")\n",
    "# 可以设置可选值\n",
    "parser.add_argument(\"-v3\", \"--verbosity3\", type = int, choices = [0, 1, 2], help = \"increase output verbosity\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "if args.verbosity3 == 2:\n",
    "    print(\"th\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### assert opt.rnn_size % self.num_directions == 0\n",
    "使用assert断言是学习python一个非常好的习惯，python assert 断言句语格式及用法很简单。在没完善一个程序之前，我们不知道程序在哪里会出错，与其让它在运行最崩溃，不如在出现错误条件时就崩溃，这时候就需要assert断言的帮助。<br>\n",
    "python assert断言是声明其布尔值必须为真的判定，如果发生异常就说明表达示为假。可以理解assert断言语句为raise-if-not，用来测试表示式，其返回值为假，就会触发异常。<br>\n",
    "assert的异常参数，其实就是在断言表达式后添加字符串信息，用来解释断言并更好的知道是哪里出了问题。格式如下：<br>\n",
    "assert expression [, arguments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "坑爹啊",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6022c7154b0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"坑爹啊\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: 坑爹啊"
     ]
    }
   ],
   "source": [
    "assert 1==2, \"坑爹啊\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### self.hidden_size = opt.rnn_size // self.num_directions\n",
    "opt.rnn_size: size of LSTM hidden states<br>\n",
    "python 2.x里面，// 是地板除，/如果有一个数是浮点数就得到小数，如果两个都是整数也是地板除。<br>\n",
    "python 3.x里面，// 是地板除，/ 不管两边是不是整数得到的都是小数。<br>\n",
    "四舍五入请用: round(5/3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1.6666666666666667, 2, 1.67)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 5 // 3 \n",
    "b = 5 / 3\n",
    "c = round(5/3)\n",
    "d = round(5/3, 2)\n",
    "a, b, c, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.ModuleList(Modules = None)\n",
    "nn.ModuleList(<br>\n",
    "&emsp;[onmt.modules.TransformerEncoder(self.hidden_size, opt)<br>\n",
    "&emsp;for i in range(opt.layers)])<br>\n",
    "transformerEncoder实现每层由两部分组成：self-attention层和positionwiseFeedForward（bottleLinear? + ReLu + dropout + residual）层<br>\n",
    "Holds submodules in a list.<br>\n",
    "ModuleList can be indexed like a regular Python list, but modules it contains are properly registered, and will be visible by all Module methods.<br>\n",
    "Parameters:\tmodules (list, optional) – a list of modules to add<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### self.rnn = getattr(nn, opt.run_type)(...)\n",
    "getattr(object, name[, default])<br>\n",
    "Return the value of the named attribute of object. name must be a string. If the string is the name of one of the object’s attributes, the result is the value of that attribute. For example, getattr(x, 'foobar') is equivalent to x.foobar. If the named attribute does not exist, default is returned if provided, otherwise AttributeError is raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-ed73608e3478>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-ed73608e3478>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    if self.encoder_layer = \"transformer\":\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 这样搭网络感觉牛牛哒。 另外nn.Module还是很陌生，\n",
    "if self.encoder_layer = \"transformer\":\n",
    "    self.transformer = nn.ModuleList(\n",
    "        [onmt.modules.TransformerEncoder(self.hidden_size, opt)\n",
    "         for i in range(opt.layers)])\n",
    "else:\n",
    "    self.rnn = getattr(nn, opt.rnn_type)(\n",
    "        input_size, self.hidden_size, num_layers = out.layers,\n",
    "        dropout = out.dropout,\n",
    "        bidirectional = opt.brnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pad_attn_mask = seq_k.data.eq(onmt.Constants.PAD).unsequeeze(1).expand(mb_size, len_q, len_k)\n",
    "- torch.eq(input, other, out=None) → Tensor: <br>\n",
    "\n",
    "Computes element-wise equality. The second argument can be a number or a tensor whose shape is broadcastable with the first argument.<br>\n",
    "return: a torch.ByteTensor containing a 1 at each location where the tensors are equal and a 0 at every other location<br>\n",
    "- torch.unsqueeze(input, dim, out=None)<br>\n",
    "\n",
    "Returns a new tensor with a dimension of size one inserted at the specified position. The returned tensor shares the same underlying data with this tensor.<br>\n",
    "A negative dim value can be used and will correspond to dim+input.dim()+1<br>\n",
    "- expand(tensor, sizes) → Tensor<br>\n",
    "\n",
    "Returns a new view of the tensor with singleton dimensions expanded to a larger size.<br>\n",
    "Tensor can be also expanded to a larger number of dimensions, and the new ones will be appended at the front.<br>\n",
    "Expanding a tensor does not allocate new memory, but only creates a new view on the existing tensor where a dimension of size one is expanded to a larger size by setting the stride to 0. Any dimension of size 1 can be expanded to an arbitrary value without allocating new memory.<br>\n",
    "另外因为multiHeadedAttention相关的那篇论文还没有看，所以并不知道为什么突然要弄个这个。回头看了在补补吧。。。。。。。。。。。。？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward中的CHECK不明白是在干什么？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### stackedCell = onmt.modules.stackedLSTM\n",
    "看了stackedLSTM，实在是没有看出区别在哪里，难道nn.LISTM实现的时候，除了第一层都是不加input的吗？？只能这么理解了。没事还是多翻翻源代码吧？？？input_feeding应该是指用额外的向量来表达已经翻译够的单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'opt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3339893780b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 类什么的，也是可以这么玩的。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"LSTM\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mstackedCell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStackedLSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstackedCell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStackedGRU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'opt' is not defined"
     ]
    }
   ],
   "source": [
    "# 类什么的，也是可以这么玩的。\n",
    "if opt.rnn_type == \"LSTM\":\n",
    "    stackedCell = onmt.modules.StackedLSTM\n",
    "else:\n",
    "    stackedCell = onmt.modules.StackedGRU\n",
    "self.rnn = stackedCell(opt.layers, input_size, opt.rnn_size, opt.dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ContextGateFactory\n",
    "工厂模式啊，好久没见过了，是这么弄的吗？？？这里主要用来调整attention和decoder state对当前循环的影响的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ContextGateFacctory(type, embeddings_size, decoder_size, attention_size, output_size):\n",
    "    gate_types = {'source': SourceContextGate,\n",
    "                 'target': TargetContextGate,\n",
    "                 'both': BothContextGate}\n",
    "    assert type in gate_types, \"not valid contextGate type: {0}\".format(type)\n",
    "    return gate_type[type](embeddings_size, decoder_size, attention_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attention\n",
    "- view(*args) → Tensor\n",
    "\n",
    "Returns a new tensor with the same data but different size.<br>\n",
    "The returned tensor shares the same data and must have the same number of elements, but may have a different size. A tensor must be contiguous() to be viewed.\n",
    "- expand_as(tensor)<br>\n",
    "\n",
    "Expands this tensor to the size of the specified tensor.\n",
    "- contiguous() → Tensor\n",
    "\n",
    "Returns a contiguous Tensor containing the same data as this tensor. If this tensor is contiguous, this function returns the original tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 明天的大概的任务\n",
    "- 看一下attention is all you need这篇论文\n",
    "- 了解一下mask的作用\n",
    "- 继续看openNMT的代码\n",
    "- 看完后，总结画个草图。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
