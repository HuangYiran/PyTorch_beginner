{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类Dataset(object):\n",
    "### 变量\n",
    "- src: 原文数据，列表类型，每个项代表一个句子（同样是列表类型。）\n",
    "- _type: 指明数据类型 默认为‘text’类型\n",
    "- tgt: 目标对象数据，类型同src，assert(len(self.src) == len(self.tgt))\n",
    "- cuda: 是否使用显卡\n",
    "- fullSize: 原文数据中句子的数量\n",
    "- batchSize: 指定Batch的大小\n",
    "- volatile: boolean类型 默认为False。指定weather the Variable should be used in inference mode\n",
    "- balance: boolean类型 默认为True 设定分Batch的方式，是否依赖句子的长度。\n",
    "- numbatches: int类型 batch的数量 = match.ceil(len(self.src)/batchSize)\n",
    "- batchs: 列表类型 每个项为一个batch\n",
    "\n",
    "### 方法\n",
    "##### __init__(self, srcData, tgtData, batchSize, cuda, volatile = False, data_type = \"text\", balance = True):\n",
    "- 设定参数\n",
    "- 如果balance为真则调用self.allocateBatch()。否则设定Batch的数量为match.ceil(len(self.src)/batchSize)\n",
    "\n",
    "##### allocateBatch(self):\n",
    "- 按照句子的长度分配原文中的句子到各个Batch之中，使得每个Batch中的句子的长度能够相同\n",
    "- 下面的操作的前提是srcData中的句子已经按照句子的长度进行排序\n",
    "- 定义变量cur_batch、cur_batch_length。对src中的每个句子进行以下处理：\n",
    "- 获得句子单词数cur_length\n",
    "- 假如cur_length不等于cur_batch_length则，如果当前的Batch里面已经有数据，则把这个Batch加入到batchs中，然后把cur_batch_length设定为当前Batch中句子的长度，把cur_batch清空\n",
    "- 把这个句子对应的下标加入到cur_batch中，如果cur_batch的大小达到BatchSize的大小，则把这个满了的Batch加入到batchs中，然后把cur_batch清空。\n",
    "- 把最后的可能的数量不足的Batch加入到batchs中。\n",
    "\n",
    "##### _batchify(self, data, align_right = False, include_lengths = False, dtype = \"text\"):\n",
    "- 这里进行padding操作，同一长度为最长的句子的长度。\n",
    "- data一般是srcData或tgtData，在这里只看数据类型是'text'的情况\n",
    "- 获得data中最长的句子的长度max_length。定义变量out，使其维度为[len(data), max_length]，并把其所有的值初始化为onmt.Constatnts.PAD\n",
    "- 根据align的方向，把data中的数据复制到out中\n",
    "- 返回out，和lengths(原来每个句子的长度)\n",
    "\n",
    "##### __getitem__(self, index):\n",
    "- 指定Batch的index，返回对应的 (srcTensor, lengths), tgtTensor, indices。此处src等的size是: [numWords, batchSize] \n",
    "- 其中src和tgt分别指该Batch中的句子包装成Variable后的结果，lengths是src进行padding之前的句子长度，indices是src根据length进行排序之前原句子的下标。\n",
    "- 根据index获得对应的Batch\n",
    "- 从src中提取Batch中index对应的句子，定义其为srcData。对其进行_batchify处理得到srcBatch和lengths。这个时候srcBatch中的句子已经经过padding处理了\n",
    "- 对tgt进行类似的处理\n",
    "- 根据Batch中句子原来的长度，对其进行排序(前面不是说过，来之前就已经排完序了吗？？？)，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = range(len(srcBatch))\n",
    "batch = (zip(indices, srcBatch) if tgtBatch is None else zip(indices, srcBatch, tgtBatch))\n",
    "# 根据lengths进行排序\n",
    "batch, lengths = zip(*sorted(zip(batch, legnts), key = lambda x: -x[1]))\n",
    "\n",
    "if tgtBatch is None:\n",
    "    indices, srcBatch = zip(*batch)\n",
    "else:\n",
    "    indices, srcBatch, tgtBatch = zip(*batch)\n",
    "\n",
    "# 定义包装方法，注意这里进行了一项操作，就是transport，所以最后输出的src等的size应该是: [numWords, batchSize] \n",
    "def wrap(b, dtype = 'text'):\n",
    "    if b is None:\n",
    "        return b\n",
    "    b = torch.stack(b, 0)\n",
    "    if dtype == 'text':\n",
    "        b = b.t().contiguous()\n",
    "    if self.cuda:\n",
    "        b = b.cuda()\n",
    "    b = Variable(b, volatile = self.volatile)\n",
    "    return b\n",
    "\n",
    "# 把长度也包装成Variable\n",
    "lengths = torch.LongTensor(lengths).view(1, -1)\n",
    "lengths = Variable(lengths, volatile = self.volatile)\n",
    "\n",
    "srcTensor = wrap(srcBatch, self._type)\n",
    "tgtTensor = wrap(tgtBatch, \"text\")\n",
    "\n",
    "return (srcTensor, lengths), tgtTensor, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __len__(self):\n",
    "- return self.numBatchs\n",
    "\n",
    "##### shuffle(self):\n",
    "- data = list(zip(self.src, self.tgt))\n",
    "- self.src, self.tgt = zip(*[data[i] for i in torch.randperm(len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2-tf",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
