{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "这里不但包括了单词的embeddings，同时还包括了各种其他的属性的embedding和position的。<br>\n",
    "不同feature之间存在三种融合的方式，即concat，sum，mlp。<br>\n",
    "模型是由多个nn.Embedding模型(针对不同的features)组合而成，根据输入和参数的不同，呈现不同数目及大小的模型。<br>\n",
    "输出的embedding已经融合了所要求的features\n",
    "#### 属性\n",
    "- self.positional_encoding: Boolean, action = 'store_true'是否使用positional_encoding.\n",
    "- self.pe: [max_len, 1, word_vec_len] max_len是最大可能的位置数量。保存的代表不同位置的向量。使向量长度为词向量的长度，是为了方便和单词的词向量进行结合。\n",
    "- self.feat_merge: choices=['concat', 'sum', 'mlp']。指定不同features embedding之间的融合方式\n",
    "- feat_exp = opt.feat_vec_exponent: 指定如果merge方式使用concat的话，融合后的向量长度，应该为对应feature的值域空间的feat_exp次方。default = 0.7\n",
    "- vocab_sizes：列表，指各种属性的值域空间的大小。\n",
    "- emb_sizes: 列表，指各种属性生成的向量的长度大小。\n",
    "- self.mlp: 当使用mlp融合时，mlp的结构\n",
    "- self.emb_luts: nn.ModuleList保存各个feature的embedding模型。\n",
    "\n",
    "#### 方法\n",
    "- __init__(self, opt, dicts, feature_dicts = None): 设置dict，feature dict的embedding模型。\n",
    "- word_lut(self): 输出单词的embedding模型\n",
    "- embedding_size(self): 返回词向量的长度\n",
    "- make_positional_encoding(self, dim, mac_len): 生成positional embedding\n",
    "- load_pretrained_vectors(self, emb_file): 顾名思义\n",
    "- merge(self, features): 融合\n",
    "- forward(self, src_input_): 运行方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "设定MT系统的Encoder部分，可以调节层的数量及每个层中节点的数量以及节点的类型。模型输入由前置的embedding模型得到。<br>\n",
    "目前支持三种类型的层：即GRU，LSTM和Transformer。<br>\n",
    "#### 属性\n",
    "- self.layers: int, encoder层的个数\n",
    "- self.num_directions: int [0, 1] 是否bidirection\n",
    "- self.hidden_size: 每个隐藏层的节点的数量，这里指的是每个方向的数量，如果是bidirection，那么总的数量应该是它的两倍。\n",
    "- self.embeddings：embedding模型\n",
    "- self.encoder_layer: choices = ['GRU', 'LSTM', 'Transformer'] 指定每个层的用的结构类型\n",
    "- self.transformer: 如果encoder_layer是Transformer，设定为对应的transformer模型（onmt.modules.TransformerEncoder）列表（每一项代表一层）\n",
    "- self.rnn: 如果encoder_layer不是Transformer，设定为对应的rnn模型（nn.LSTM/nn.GRU）列表\n",
    "\n",
    "#### assert\n",
    "assert opt.rnn_size % self.num_direction == 0\n",
    "\n",
    "#### 方法\n",
    "__init__(self, opt, dicts, feature_dicts = None)\n",
    "forward(self, intput, lengths = None, hidden = None): length是指batch_size，hidden是隐藏层的初始值。\n",
    "\n",
    "#### 图\n",
    "还木画"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "设定MT模型的Decoder部分，可以设定层的数量及每个层中节点的数量以及节点的类型，attention的类型。<br>\n",
    "目前支持的层的类型有：GRU，LSTM，Transformer<br>\n",
    "目前支持的attention的类型有：dot, general, mlp, copy<br>\n",
    "目前支持的结合context的方式有：source, target, both<br>\n",
    "\n",
    "#### 属性\n",
    "- self.layers: int, decoder层的个数，encoder和decoder层数相同\n",
    "- self.decoder_layer: int, decoder每个层的类型\n",
    "- self._coverage: action='store_true' 指定attention是否要使用coverage。用额外的向量来记录已经翻译到什么地方，从而实现coverage。\n",
    "- self.hidden_size: 因为不用考虑bidrectional所以等于opt.rnn_size\n",
    "- self.input_feed: 指定是否使用额外的context信息作为输入信息，如果是将扩充input_size(原始为word_vec_size)，default = 1\n",
    "- self.transformer: 如果decoder_layer是Transformer，设定为对应的Transformer模型（onmt.modules.TransformerDecoder()）列表\n",
    "- self.rnn: 如果decoder_layer不是Transformer，设定为对应的rnn模型(onmt.modules.StackedLSTM/GRU)列表\n",
    "- self.context_gate: 指定使用的context_gate的类型，choices = ['source', 'target', 'both']。当使用input_feed时，context_gate作为权值，权衡contxt和原始input之间的比重\n",
    "- self.dropout: dropput模型。\n",
    "- self.attn: attention模型，可以指定attention的类型，choices = ['dot', 'general', 'mlp']\n",
    "- self._copy: 指定是否训练copy attention模型（onmt.modules.GlobalAttention）， action = 'store_true'\n",
    "\n",
    "#### 方法\n",
    "- __init__(self, opt, dicts)<br>\n",
    "- forward(self, input, src, context, state): context [src_len, batch, rnn_size], state是一个用于初始化decode的对象\n",
    "\n",
    "#### 图\n",
    "还木画"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMTModel\n",
    "\n",
    "#### 属性\n",
    "- self.multigpu: boolean是否使用multi gpu\n",
    "- self.encoder\n",
    "- self.decoder\n",
    "\n",
    "#### 方法\n",
    "- __init__(self, encoder, decoder, multigpu = False)\n",
    "- _fix_enc_hidden(sellf, h): change [layers*directions, batch, dim] to [layers, batch, directions*dim]\n",
    "- init_decoder_state(self, context, enc_hidden)\n",
    "- forward(self, src, tgt, lengths, dec_state = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNDecoderState(DecoderState)\n",
    "\n",
    "#### 属性\n",
    "self.hidden: rnn中的可训练变量[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dict features不是很了解，有什么features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## onmt.Loss.py\n",
    "### NMTCriterion\n",
    "设置loss function为NLLLoss，初始化weight使得忽略对应特殊字符。\n",
    "#### 参数\n",
    "- vocabulary Size\n",
    "- opt\n",
    "#### 返回\n",
    "Loss function\n",
    "\n",
    "### ShardVariable\n",
    "？？？？不知道是要干什么的？？？\n",
    "\n",
    "### collectGrads\n",
    "？？？？搞不懂啊，和上一个方法相关，等遇到了在看？？？\n",
    "\n",
    "### class Statistics\n",
    "计算各种统计数据\n",
    "#### 初始化参数\n",
    "- loss = 0 \n",
    "- n_words = 0\n",
    "- n_correct = 0\n",
    "\n",
    "#### 属性(都表示字面意思)\n",
    "- self.loss = loss\n",
    "- self.n_words = n_words\n",
    "- self.n_correct = n_correct\n",
    "- self.n_str_words = 0\n",
    "- self.start_time = time.time() (import time)\n",
    "\n",
    "#### 方法\n",
    "- _init_(self, loss = 0, n_words = 0, n_correct = 0):\n",
    "- update(self, stat): stat应该是statistics类型的，把stat中的属性，加到目前的属性中\n",
    "- accuracy(self): 返回$100*(self.n_correct / float(self.n_words)$\n",
    "- ppl(self): 返回$math.exp(min(self.loss/ self.n_words, 100))$\n",
    "- elapsed_time(self): 放回当前时间减去刚生成这个实例时的时间\n",
    "- output(self, epoch, batch, n_batches, start): 统计前面方法中得到的信息，进行输出\n",
    "- log(self, prefix, experiment, optim): 把信息加到experiment中\n",
    "\n",
    "### MemoryEfficientLoss\n",
    "这个先跳了？？？？？应该先看一下什么是copygenerator，否则实在看不懂啊？？？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## onmt.modules.CopyGenerator\n",
    "###  class CopyGenerator(nn.Module)\n",
    "暂时不知道为什么需要copyGenerator<br>\n",
    "Generator module that additionally considers copying words directly from the source\n",
    "#### 属性\n",
    "- self.linear: [rnn_size, tgt_dict.size()]，用于计算目标词汇表各个单词的概率\n",
    "- self.linear_copy: [rnn_size, 1]，用于计算copy的概率\n",
    "- self.src_dict\n",
    "- self.tgt_dict: target dict\n",
    "#### 方法\n",
    "- __init__(self, opt, src_dict, tgt_dict)\n",
    "- forward(self, hidden, attn, verbose = False)： args:hidden(FLoatTensor): (tgt_len \\* batch) x hidden为什么是tgt_length \\* batch，tgt_length是什么？？怀疑他表示目标句子的长度<br>\n",
    "计算$P(w) = p(z=1)p_{copy}(w|z=0) 和 p(z=0)*P_{softmax}(w|z=0)$，其中p(z=1)表示copy的概率，其值由输入hidden确定。$p_{softmax}$表示目标词汇表中，各个单词出现的概率，其值由输入hidden确定。$p_{copy}$就是输入attn：(tgt_len*length) x src_len\n",
    "- _debug_copy(self, src, copy, prob, out_prob, attn, mal_attn): 不是很懂这个方法是为了干什么？？？\n",
    "\n",
    "### CopyCriterion(probs, attn, targ, align, eps = 1e-12): 看不懂啊？？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## onmt.optim.py\n",
    "### class Optim(object)\n",
    "- self.last_ppl: 当前的ppl值\n",
    "- self.lr: learning rate\n",
    "- self.max_grad_norm: 设置max_grad_norm，用于clip_grad_norm.\n",
    "- self.method: 方法\n",
    "- self.lr_decay: learning rate decay??\n",
    "- self.start_decay_at\n",
    "- self.start_decay: boolean\n",
    "- self._step: 记录目前step的次数，用于辅助计算decay_lr，具体原理不懂？\n",
    "- self.betas: = [beta1, beta2]用于Adam。coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999))\n",
    "- self.opt: opt\n",
    "- self.optimizer: 优化器\n",
    "#### 方法\n",
    "- __init__(self, method, lr, max_grad_norm, lr_decay = 1, start_decay_at = None, beta1 = 0.9, beta2 = 9.8, opt = None)\n",
    "- set_parameters(self, params): 设置optimizer的方法，包括：sgd, adagrad, adadelta, adam\n",
    "- _setRate(self, lr): 设置optimizer的learning rate\n",
    "- step(self): 加入了根据是否设置了decay_method, 调整learning rate\n",
    "- updateLearningRate(self, ppl, epoch): decay learning rate if val perf does not improve or we hit the start_decay_at limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## onmt.IO.py\n",
    "### extractFeatures(tokens)\n",
    "输入的tokens是一个列表，其中每个项表示单词及其对应的词性，他们通过‘|’分隔开。该方法的功能主要是提取每个token中的word和features。<br>\n",
    "应该注意的是，每个单词可以有多个features，但是每个单词的features的总数应该是相同的。<br>\n",
    "该方法返回words, features(列表，len(features)代表feature的个数), numFeatures\n",
    "\n",
    "### merge_vocabs(vocabs, vocab_size = None)\n",
    "合并多个词汇表，一般词汇表从特定的文档汇中，归纳得到，不同文档得到的词汇表之间，存在差异。这个方法，主要用来合并这些词汇表。vocab_size指定最后生成的词汇表的大小，None的话就是没有限定。<br>\n",
    "用try: merged[word] += count except: merged[word] = 0，也是可以的，但不知道优劣在哪里。<br>\n",
    "返回用到torchtext，不知道是在干什么？？还有vocab也是torchtext里面的，所以只能表示懵逼了？？？\n",
    "\n",
    "### class OrderedIterator(torchtext.data.Iterator)\n",
    "父类是什么鬼？？？\n",
    "\n",
    "### class ONMTDataset(torchtext.data.Dataset):\n",
    "父类是什么鬼？？？\n",
    "以torchtext.data.Dataset为基础，打造一个使用与MT任务的Dataset类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train.py\n",
    "#### 读取参数并做一些设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//提取选项\n",
    "parser = argparse.ArgumentParser(description='train.py')\n",
    "opt = parser.parse_args() \n",
    "\n",
    "// 设定层数和embedding vector的长度。\n",
    "// 若没有特殊指定，默认为2和500\n",
    "opt.src_word_vec_size = opt.tgt_word_vec_size = opt.word_vec_size\n",
    "opt.enc_layers = opt.dec_layers = opt.layers\n",
    "\n",
    "// 设定是否用双向encoder\n",
    "opt.brnn = (opt.encoder_type == 'brnn')\n",
    "\n",
    "// 设定seed，并不知道这一步，优在哪里？？？\n",
    "if opt.seed>0: torch.manual_seed(opt.seed)\n",
    "    \n",
    "// 若存在n卡，那么最好就用显卡跑吧，关于显卡的一切，目前表示未知？？？\n",
    "if torch.cuda.is_available() and opt.gpuid: \n",
    "    cuda.set_device(opt.gpuid[0]) \n",
    "    if opt.seed > 0: torch.cuda.manual_seed(opt.seed)\n",
    "        \n",
    "// 设定远端log服务器crayon，若当前experiment已经存在，则覆盖之。具体使用同样表示未知？？？\n",
    "if opt.exp_host != \"\":\n",
    "    from pycrayon import CrayonClient\n",
    "    cc = CrayonClient(hostname = opt.exp_host)\n",
    "    if opt.exp in cc.get_experiment_names(): cc.remove_experiment(opt.exp)\n",
    "    experiment = cc. create_experiment(opt.exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实现main方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// 读取数据到cpu, 不知道什么样的数据才能使用这种方法读取，只有通过torch.store的数据吗？\n",
    "train = torch.load(opt.data + '.train.pt', pickle_module=dill)\n",
    "f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
