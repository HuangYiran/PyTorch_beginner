{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类Translator(object):\n",
    "### 变量\n",
    "- opt: 为OpenNMT-py/translate.py中的超参数\n",
    "- tt: = torch.cuda if opt.cuda else torch\n",
    "- beam_accum: 词典类，项目包括：predicted_ids, beam_parent_ids, scores, log_probs。分别都对应着一个列表类\n",
    "- model_opt: = checkpoint['opt']\n",
    "- src_dict: = checkpoint['dicts']['src']\n",
    "- tgt_dict: = checkpoint['dicts']['tgt']\n",
    "- _type: 模型中encoder的类型，如果模型中没有设定，则返回‘text’\n",
    "- model: 用于翻译的模型 参数从checkpoint中获得 onmt.Models.NMTModel(encoder, decoder)\n",
    "\n",
    "### 方法\n",
    "##### __init__(self, opt):\n",
    "- Inputs\n",
    "    - opt: translate.py中的超参数，主要指定使用哪个训练好的模型，以及用于翻译的源文件的路径\n",
    "- 主要的任务是构建模型。并读取已训练的模型的参数\n",
    "- 读取checkpoint: = torch.load(opt.model, map_location = lambda storage, loc: storage)。lambda后面的是干嘛用的表示不懂？？\n",
    "- 从checkpoint中读取模型的参数model_opt = checkpoint['opt']\n",
    "- 从checkpoint中读取原文和目标文的词汇表src_dict tgt_dict\n",
    "- 根据model_opt中encoder的类型，设定encoder模型 = onmt.Models.Encoder(model_opt, self.src_dict)\n",
    "- 根据model_opt设定decoder模型 = onmt.Models.Decoder(model_opt, self.tgt_dict) <font color=gray>应该具体看一下</font>\n",
    "- 根据上面的encoder和decoder，建立MT模型model = onmt.Models.NMTModel(encoder, decoder) <font color=gray>应该具体看一下</font>\n",
    "- 还需自己额外补一个generator，是一个从隐藏层到目标词汇表的线性映射，并对结果进行logSoftmax处理\n",
    "- 根据checkpoint设置model的参数 model.load_state_dict(checkpoint['model'])\n",
    "- 根据checkpoint设置generator的参数 generator.load_state_dict(checkpoint['model'])\n",
    "- 如果有设置cuda，则开启model和generator的cuda否则则使用cpu <font color=red>这个表示不懂</font>\n",
    "- 启动模型model的evaluation模式\n",
    "\n",
    "##### initBeamAccum(self):\n",
    "- 初始化beam_accum\n",
    "\n",
    "##### _getBachSize(self, batch):\n",
    "- 根据_type的类型返回batch的大小\n",
    "\n",
    "##### buildData(self, srcBatch, goldBatch):\n",
    "- 主要完成把数据集中的单词转换成对应的index。然后把数据都扔进dataset类型里面进行包装（padding，根据batchsize的大小，划分成多个Batch）。\n",
    "- 注意 这里的操作要求和preprocess中的一样。\n",
    "- 通过词典类把srcBatch中的单词，转换成对应的index\n",
    "- 如果goldBatch也存在，则对其作相同的处理（这里的goldBatch指reference译文的意思？）\n",
    "- 返回Dataset类型的实例\n",
    "\n",
    "#### buildTargetTokens(self, pred, src, attn):\n",
    "- 把生成的index序列pred，转化成单词序列（去掉最后的EOS符号）\n",
    "- attn不知道是什么，被用与replace_unk的操作之中？？？？\n",
    "\n",
    "##### translateBactch(self, srcBatch, tgtBatch):\n",
    "- batch size is in different location depending on data这句话不知道是什么意思？？？\n",
    "- 运行model.encoder(srcBatch)得到encStates和context\n",
    "- 下面内容放到代码中进行解说"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainslateBatch(self, srcBatch, tgtBatch):\n",
    "    beamSize = self.opt.beam_size\n",
    "    # 1) run the encoder on the src\n",
    "    # 其中encoder的输入的size是：[seq_len, batch, input_size], 这里seq_len即是numWords\n",
    "    # 其中context是[seq_len, batch, hidden_size * num_directions]\n",
    "    # encStates是一个tupel，他包括:\n",
    "    #  - h_0 (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t=seq_len\n",
    "    #  - c_0 (num_layers * num_directions, batch, hidden_size): tensor containing the cell state for t=seq_len\n",
    "    encStates, context = self.model.encoder(srcBatch)\n",
    "    \n",
    "    # 这里的srcBatch原本是dataset的输出，所以应该是(src, lengths)，下面这一步取出其中src的内容。\n",
    "    srcBatch = srcBatch[0]\n",
    "    batchSize = self._getBatchSize(srcBatch)\n",
    "    \n",
    "    # 获得RNN每层的节点数\n",
    "    rnnSize = context.size(2)\n",
    "    \n",
    "    # 转换encState的维度为：[layers * batch * (directions * dim)]\n",
    "    encStates = (self.model._fix_enc_hidden(encStates[0]), self.model_fix_enc_hidden(encStates[1]))\n",
    "    \n",
    "    decoder = self.model.decoder\n",
    "    attentionLayer = decoder.attn\n",
    "    \n",
    "    # 如果类型为text，且batchSize大于1，则使用mask，原因未知？？？？？\n",
    "    # 这个mask 将被用于decoder中的Attention，目的是使得attention能够忽略掉输入句子的padding的部分内容。\n",
    "    # 但为什么仅在batchSize大于1的时候，才使用呢？？？\n",
    "    useMasking = (self._type == 'text' and batchSize > 1)\n",
    "    padMask = None\n",
    "    if useMasking:\n",
    "        padMask = srcBatch.data.eq(onmt.Constants.PAD).t() #标记pad的内容\n",
    "    \n",
    "    def mask(padMask):\n",
    "        if useMasking:\n",
    "            attentionLayer.appleMask(padMask)\n",
    "    \n",
    "    # 如果有参考译文，则进行打分\n",
    "    # Tensor的new()创建一个相同类型的Tensor，这里初始每个句子的分数为0\n",
    "    goldScores = context.data.new(batchSize).zero_()\n",
    "    if tgtBatch is not None:\n",
    "        decStates = encStates\n",
    "        # 初始化一个decoder的输出，\n",
    "        decOut = self.model.make_init_decoder_output(context)\n",
    "        mask(padMask)\n",
    "        initOutput = self.model.make_init_decoder_output(context)\n",
    "        # decoder的输出使outputs, hidden, atten, 前两者同一般的rnn输出，atten是nn.softmax()的输出\n",
    "        # 关于tgtBatch，应该是一个size: [numWords, batchSize]的Variable(Datasetl类里面进行了转换)\n",
    "        # globalAttention的参数是input: batch x hidden_size，context: batch x seq_len x hidden_size。\n",
    "        # 对应输出的attn是batch X seq_len\n",
    "        # 所以decoder的输出output, hidden, attn的size应该分别是：\n",
    "        # [seq_len X batch X hidden_size], [num_layers X batch X hidden_size], [batch X seq_len]\n",
    "        decOut, decStates, attn = self.model.decoder(tgtBatch[:-1], decStates, context, initOutput)\n",
    "        for dec_t, tgt_t in zip(decOut, tgtBatch[1:].data):\n",
    "            gen_t = self.model.generator.forward(dec_t) \n",
    "            tgt_t = tgt_t.unsqueeze(1)\n",
    "            scores = gen_t.data.gather(1, tgt_t) # http://www.cnblogs.com/YiXiaoZhou/p/6387769.html\n",
    "            scores.masked_fill_(tgt_t.eq(onmt.Constants.PAD), 0)\n",
    "            goldScores += scores\n",
    "    \n",
    "    # 使用decoder来生成目标句子，\n",
    "    # 根据beamSize扩张前面的tensor，\n",
    "    # 这里原来的context是[seq_len, batch, hidden_size * num_directions]\n",
    "    # repeat之后的context就是[seq_len, batch * beamSize, hidden_size * num_directions]\n",
    "    context = Variable(context.data.repeat(1, beamSize, 1))\n",
    "    decStates = (Variable(encStates[0].data.repeat(1, beamSize, 1)),\n",
    "                 Variable(encStates[1].data.repeat(1, beamSize, 1)))\n",
    "    \n",
    "    # 生成batchSize个onmt.Beam对象, 注意是batchSize，每个句子一个\n",
    "    beam = [onmt.Beam(beamSize, self.opt.cuda) for k in range(batchSize)]\n",
    "    \n",
    "    decOut = self.model.make_init_decoder_output(context)\n",
    "    \n",
    "    if useMasking:\n",
    "        padMask = srcBatch.data.eq(onmt.Constant.PAD).t().unsqueeze(0).repeat(beamSize, 1, 1)\n",
    "        \n",
    "    batchIdx = list(range(batchSize))\n",
    "    remainingSents = batchSize\n",
    "    \n",
    "    for i in range(self.opt.max_sent_length):\n",
    "        # 遍历最长句子长度，也就是说下面的操作是针对每个单词进行的了？？\n",
    "        mask(padMask)\n",
    "        # prepare decoder input？？b.getCurrentState()的返回是size (beamSize), stack后是(batch_size, beamSize)\n",
    "        # t()后是(beamSize, batchSize), view()后就成了(1, beamSize*batchSize)\n",
    "        # 这里的batchSize是batch中为未处理完的句子的数量，beamSize中记录的是上一轮中得到的单词。\n",
    "        # t()处理是为了来自相同batch的家伙，可以站在一起\n",
    "        input = torch.stack([b.getCurrentState() for b in beam\n",
    "                             if not b.done()]).t().contiguous().view(1, -1)\n",
    "        # 官方给出这里的decOut的size是[1 X (beam*batch) X numWords] 这里的numWord应该是值rnn_size，和后面的指的应该不是一个东西\n",
    "        decOut, decStates, attn = self.model.decoder(Variable(input, volatile = True),\n",
    "                                                    decStates, context, decOut)\n",
    "        decOut = decOut.squeeze(0)\n",
    "        # 疯了 这个generator究竟是哪里冒出来的，Forward在哪里定义的，为什么输出是(batch, numWords), \n",
    "        # 根据前面说的numWords应该就是rnn_size了。但根据推测，这里说的应该是tgt_dict.size\n",
    "        out = self.model.generator.forward(decOut)\n",
    "        \n",
    "        # generator的输出应该是(batch X beam, tgt_dict.size)啊，为什么下面有变回numWords了，？？？？？\n",
    "        # batch X beam X numWords, 前面把batchSize赋值给remainingSents了。\n",
    "        wordLk = out.view(beamSize, remainingSents, -1).transpose(0, 1).contiguous()\n",
    "        # batch X beam X seq_len\n",
    "        attn = attn.view(beamSize, remainingSents, -1).transpose(0, 1).contiguous()\n",
    "        \n",
    "        active = []\n",
    "        \n",
    "        for b in range(batchSize):\n",
    "            if beam[b].done:\n",
    "                # 已经处理完的句子就不用管了\n",
    "                continue\n",
    "            \n",
    "            # batchIdx在后面被重新定义为字典类型，是从batchSize中的编码到remainingSents中编码的一个映射。\n",
    "            # 首轮batchSize等于remainingSents所以就是一个列表\n",
    "            idx = batchIdx[b]\n",
    "            # advance, 到句末则返回True，否则则说面还没处理完，那么就把他加入到active列表中\n",
    "            if not beam[b].advance(wordLk.data[idx], attn.data[idx]):\n",
    "                active += [b]\n",
    "            \n",
    "            for decState in decStates: # iterate over h, c\n",
    "                # layers X beam*sent X dim, sent指上一轮中，处理的句子的数量。\n",
    "                # 提取Batch中对应句子的decState内容\n",
    "                sentStates = decState.view(-1, beamSize,\n",
    "                                           remainingSents,\n",
    "                                           decStates.size(2))[:, :, idx]\n",
    "                # 1维代表的是beamSize，getCurrentOrigin得到的是当前的prevKs。\n",
    "                sentState.data.copy_(sentStates.data.index_select(1,\n",
    "                                                                 beam[b].getCurrentOrigin()))\n",
    "            \n",
    "        if not active:\n",
    "            break\n",
    "        \n",
    "        # in this section, the sentences that are still active are compacted\n",
    "        # so that the decoder is not run on completed sentences\n",
    "        activeIdx = self.tt.LongTensor([batchIdx[k] for k in active])\n",
    "        batchIdx = {beam: idx for idx, beam in enumerate(active)} # 注意这里\n",
    "        \n",
    "        def updateActive(t):\n",
    "            # select only the remaining active sentences\n",
    "            view = t.data.view(-1, remainingSents, rnnSize)\n",
    "            newSize = list(t.size())\n",
    "            # 为什么不是直接等于len(activeIdx)，都是整数，这么弄有差吗？？？？\n",
    "            newSize[-2] = newSize[-2] * len(activeIdx) // remainingSents\n",
    "            return Variable(view.index_select(1, activeIdx).view(*newSize), volatile = True)\n",
    "        \n",
    "        decStates = (updateActive(decStates[0]),\n",
    "                     updateActive(decStates[1]))\n",
    "        decOut = updateActive(decOut)\n",
    "        context = updateActive(context)\n",
    "        if useMasking:\n",
    "            padMask = padMask.index_select(1, activeIdx)\n",
    "        \n",
    "        remainingSents = len(active)\n",
    "    \n",
    "    # 下面没有好认真的看。。。\n",
    "    # package everything up \n",
    "    allHyp, allScores, allAttn = [], [], []\n",
    "    n_best = self.opt.n_best\n",
    "    \n",
    "    for b in range(batchSize):\n",
    "        # ks是index 指向原列表中的坐标\n",
    "        scores, ks = beam[b].sortBest()\n",
    "        \n",
    "        allScores += [scores[:n_best]]\n",
    "        hyps, attn = zip(*[beam[b].getHyp(k) for k in ks[:n_best]])\n",
    "        allHyp += [hyps]\n",
    "        if useMasking:\n",
    "            valid_attn = srcBatch.data[:, b].ne(onmt.Constants.PAD).nonzero().squeeze(1)\n",
    "            attn = [a.index_select(1, valid_attn) for a in attn]\n",
    "        allAttn += [attn]\n",
    "        \n",
    "        if self.beam_accum:\n",
    "            self.beam_accum[\"beam_parent_ids\"].append([t.tolist() for t in beam[b].prevKs])\n",
    "            self.beam_accum[\"scores\"].append([[\"%4f\" %s for s in t.tolist()] for t in beam[b].allScores][1:])\n",
    "            self.beam_accum[\"predicted_ids\"].append([[self.tgt_dict.getLabel(id) for id in t.tolist()]\n",
    "                                                     for t in beam[b].nextYs][1:])\n",
    "    \n",
    "    return allHyp, allScores, allAttn, goldScores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### translate(self, srcBatch, goldBatch):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def translate(self, srcBatch, goldBatch):\n",
    "    # 把单词转化成对应的index，然后放进dataset中进行包装\n",
    "    dataset = self.buildData(srcBatch, goldBatch)\n",
    "    # 获得第一个Batch\n",
    "    src, tgt, indices = dataset[0]\n",
    "    batchSize = self._getBatchSize(src[0])\n",
    "    \n",
    "    # 扔到translateBatch方法里面进行翻译, 这里src，tgt都是tensor类型维度为batchSize*numWord\n",
    "    pred, predScore, attn, goldScore = self.translateBatch(src, tgt)\n",
    "    # 根据indices，对上面得到的结果进行排序。使顺序回到通过lengths进行排序之前。\n",
    "    pred, predScore, attn, glodScore = list(zip(*sorted(zip(pred, predScore, attn, goldScore, indices),\n",
    "                                                        key = lambda x: x[-1])))[:-1]\n",
    "    \n",
    "    # 把indexes转换会对应的单词\n",
    "    predBatch = []\n",
    "    for b in range(batchSize):\n",
    "        predBatch.append(\n",
    "            [self.buildTargetTokens(pred[b][n], srcBatch[b], attn[b][n])\n",
    "             for n in range(self.opt.n_best)]\n",
    "        )\n",
    "    return predBatch, predScore, goldScore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2-tf",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
