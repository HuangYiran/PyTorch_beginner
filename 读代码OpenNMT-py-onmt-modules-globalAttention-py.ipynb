{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类GlobalAttention\n",
    "### 变量\n",
    "- linear_in: nn.Linear(rnn_size, rnn_size, bias = False)\n",
    "- linear_context: nn.Linear(rnn_size, rnn_size, bias = False)\n",
    "- sm: nn.softmax()\n",
    "- linear_out: nn.Linear(rnn_size * 2, rnn_size, bias = False)\n",
    "- linaer_to_one: nn.Linear(rnn_size, 1, bias = True)\n",
    "- tanh = nn.Tanh()\n",
    "- mlp_tanh = nn.Tanh()\n",
    "- mask: 用于屏蔽padding的单词\n",
    "- linear_cg: nn.Linear(dim * 2, dim, bias = True)\n",
    "- sigmoid_cg: nn.Sigmoid()\n",
    "\n",
    "### 方法\n",
    "##### __init__(self, dim):\n",
    "- dim一般是rnn_size\n",
    "\n",
    "##### defineMask(self, mask):\n",
    "- self.mask = mask \n",
    "    - mask (batch, seq_len)\n",
    "\n",
    "##### forward(self, input, context):\n",
    "- Inputs: input, context\n",
    "    - input (batch, hidden_size) 代表Query\n",
    "    - context (batch, seq_len, hidden_size * num_directions) 代表key和value\n",
    "        -???双向怎么办？这里好像没有考虑这种情况，至少没在这里进行考虑\n",
    "- Outputs: output, attn\n",
    "    - output (batch, rnn_size)\n",
    "    - attn (batch, seq_len)\n",
    "- 下面开始看代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward(self, input, context):\n",
    "    # 得到batch size， seq_len, rnn_size \n",
    "    bsize = context.size(0)\n",
    "    seq_length = context.size(1)\n",
    "    dim = context.size(2)\n",
    "    \n",
    "    # project the hidden state(query)\n",
    "    targetT = self.linear_in(input).unsqueeze(1) # batch X 1 X rnn_size\n",
    "    \n",
    "    # project the context (keys and values)\n",
    "    reshaped_ctx = context.contiguous().view(bsize * seq_len, dim)\n",
    "    projected_ctx = self.linear_context(reshapred_ctx)\n",
    "    projected_ctx = projected_ctx.view(bsize, seq_length, dim)\n",
    "    \n",
    "    # MLP attention model\n",
    "    repeat = targetT.expand_as(projected_ctx)\n",
    "    sum_query_ctx = repeat + projected_ctx\n",
    "    sum_query_ctx = sum_query_ctx.view(bsize*seq_len, dim)\n",
    "    \n",
    "    mlp_input = self.mlp_tanh(sum_query_ctx)\n",
    "    mlp_output = self.linear_to_one(mlp_input)\n",
    "    \n",
    "    mlp_output = mlp_output.view(bsize, seq_len, 1)\n",
    "    attn = mlp_output.squeeze(2) # 不是点乘，而是相加后通过tanh来得到attn，也是长见识了。暂时不知道他的有点是什么？？？？\n",
    "    \n",
    "    # get attention\n",
    "    if self.mask is not None:\n",
    "        attn.data.masked_fill_(self.mask, -float('inf'))\n",
    "    attn = self.sm(attn)\n",
    "    attn3 = attn.view(attn.size(0), 1, attn.size(1)) # batch X 1 X seq_len\n",
    "    \n",
    "    # (batch, 1, seq_len) * (batch, seq_len, rnn_size) => (batch, 1, rnn_size)即得到各个时间节点隐藏层节点的加权和\n",
    "    weightedContext = torch.bmm(attn3, context).squeeze(1) # batch X dim\n",
    "    \n",
    "    # ContextGate, 它由input和weightedContext两部分共同决定。\n",
    "    # cat input(batch, dim)\n",
    "    contextCombined = torch.cat((weightedContext, input), 1) # batch X dim*2\n",
    "    contextGate = self.sigmoid_cg(self.linear_cg(contextCombined)) # batch X dim\n",
    "    inputgate = 1 - contextGate\n",
    "    \n",
    "    gatedContext = weightdContext * contextGate\n",
    "    gatedInput = input * inputGate\n",
    "    gatedContextCombinded = torch.cat((gatedContext, gatedInput), 1) # batch X dim *2\n",
    "    contextOutput = self.tanh(self.linear_out(gatedContextCombined)) # batch X dim\n",
    "    \n",
    "    return contextOutput, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2-tf",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
